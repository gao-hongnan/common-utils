{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer\n",
        "\n",
        "```{tableofcontents}\n",
        "\n",
        "```\n",
        "\n",
        "## Notations\n",
        "\n",
        "> Mostly follow Lilian's notations, except for $d_k$, $d_q$ and $d_v$, which we\n",
        "> will stay consistent with the paper.\n",
        "\n",
        "### Dimensions and Indexing\n",
        "\n",
        "Here we list common dimensions and indexing used in the Transformer model.\n",
        "Dimensions and indexing pertaining to attention will be listed in the\n",
        "[Attention Notations](#attention-notations) section.\n",
        "\n",
        "- $D$: embedding dimension. In the paper it is denoted as $d_{\\text{model}}$.\n",
        "  - $d$: index of an element in the embedding vector.\n",
        "- $L$: sequence length.\n",
        "  - $i$: index of a token in the sequence.\n",
        "- $V$: vocabulary size.\n",
        "  - $j$: index of a word in the vocabulary.\n",
        "\n",
        "### General Notations\n",
        "\n",
        "- $\\mathcal{V}$: is the set of all words in the vocabulary defined as:\n",
        "\n",
        "  $$\n",
        "  \\mathcal{V} = \\{v_1, v_2, ..., v_V\\}\n",
        "  $$\n",
        "\n",
        "  where\n",
        "\n",
        "  - $V$: is the size of the vocabulary, also denoted as $|\\mathcal{V}|$.\n",
        "  - $v_j$: is a unique word in the vocabulary $\\mathcal{V}$.\n",
        "  - $j$: is the index of a word in the vocabulary $\\mathcal{V}$.\n",
        "\n",
        "- $\\mathbf{X}$: is the input sequence defined as:\n",
        "\n",
        "  $$\n",
        "  \\mathbf{X} = (x_1, x_2, ..., x_L)\n",
        "  $$\n",
        "\n",
        "  where\n",
        "\n",
        "  - $L$: is the sequence length.\n",
        "  - $x_{i}$: is a token at position $i$ in the sequence. Each $x_{i}$ is a token\n",
        "    represented as an integer from the set ${0, 1, ..., V-1}$.\n",
        "  - $i$: is the index of a token in the sequence $\\mathbf{X}$.\n",
        "\n",
        "- $\\mathbf{O}$: one-hot representation of the input sequence $\\mathbf{X}$. This\n",
        "  is a $L \\times V$ matrix, where each row represents a token in the sequence\n",
        "  and each column corresponds to a unique word in the vocabulary $\\mathcal{V}$.\n",
        "\n",
        "  $$\n",
        "  \\begin{aligned}\n",
        "  \\mathbf{O} &= \\begin{bmatrix} o_{1,1} & o_{1,2} & \\cdots & o_{1,V} \\\\ o_{2,1} & o_{2,2} & \\cdots & o_{2,V} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ o_{L,1} & o_{L,2} & \\cdots & o_{L,V} \\end{bmatrix} \\in \\mathbb{R}^{L \\times V} \\\\\n",
        "  &= \\begin{bmatrix} \\text{---} & \\mathbf{o}_{1, :} & \\text{---} \\\\ \\text{---} & \\mathbf{o}_{2, :} & \\text{---} \\\\ & \\vdots & \\\\ \\text{---} & \\mathbf{o}_{L, :} & \\text{---} \\end{bmatrix} \\in \\mathbb{R}^{L \\times V}\n",
        "  \\end{aligned}\n",
        "  $$\n",
        "\n",
        "  where\n",
        "\n",
        "  - $L$: is the sequence length.\n",
        "  - $V$: is the vocabulary size.\n",
        "  - $o_{i, j}$: is the one-hot encoded element at position $i, j$. For a given\n",
        "    token $x_i$ at the $i$-th position in the sequence $\\mathbf{X}$, if\n",
        "    $f_{\\text{stoi}}(x_i)=j$, then the element at position $j$ in the one-hot\n",
        "    vector for token $x_i$ is 1, and all other elements are 0.\n",
        "  - $\\mathbf{o}_{i, :}$: is the one-hot encoded vector for the token $x_i$ at\n",
        "    the $i$-th position in the sequence $\\mathbf{X}$. This row form is more\n",
        "    important than column form.\n",
        "\n",
        "- $\\mathbf{E}$: is the embedding matrix defined as:\n",
        "\n",
        "  $$\n",
        "  \\mathbf{E} = \\begin{bmatrix} e_{1,1} & e_{1,2} & \\cdots & e_{1,D} \\\\ e_{2,1} & e_{2,2} & \\cdots & e_{2,D} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ e_{V,1} & e_{V,2} & \\cdots & e_{V,D} \\end{bmatrix} \\in \\mathbb{R}^{V \\times D}\n",
        "  $$\n",
        "\n",
        "  where\n",
        "\n",
        "  - $V$: is the vocabulary size.\n",
        "  - $D$: is the embedding dimension.\n",
        "  - $e_{j, d}$: is the embedding element at position $j, d$. For a word $v_j$ in\n",
        "    the vocabulary $\\mathcal{V}$, the corresponding row in $\\mathbf{E}$ is the\n",
        "    embedding vector for that word.\n",
        "\n",
        "- $\\mathbf{Z}$: is the output tensor of the embedding layer, obtained by matrix\n",
        "  multiplying $\\mathbf{O}$ with $\\mathbf{E}$, and it is defined as:\n",
        "\n",
        "  $$\n",
        "  \\mathbf{Z} = \\mathbf{O} \\cdot \\mathbf{E}\n",
        "  $$\n",
        "\n",
        "  $$\n",
        "  \\begin{aligned}\n",
        "  \\mathbf{Z} &= \\mathbf{O} \\cdot \\mathbf{E} \\\\\n",
        "  &= \\begin{bmatrix} z_{1,1} & z_{1,2} & \\cdots & z_{1,D} \\\\ z_{2,1} & z_{2,2} & \\cdots & z_{2,D} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ z_{L,1} & z_{L,2} & \\cdots & z_{L,D} \\end{bmatrix} \\in \\mathbb{R}^{L \\times D} \\\\\n",
        "  &= \\begin{bmatrix} \\text{---} & \\mathbf{z}_{1,:} & \\text{---} \\\\ \\text{---} & \\mathbf{z}_{2,:} & \\text{---} \\\\ & \\vdots & \\\\ \\text{---} & \\mathbf{z}_{L,:} & \\text{---} \\end{bmatrix} \\in \\mathbb{R}^{L \\times D}\n",
        "  \\end{aligned}\n",
        "  $$\n",
        "\n",
        "  where\n",
        "\n",
        "  - $L$: is the sequence length.\n",
        "  - $D$: is the embedding dimension.\n",
        "  - $z_{i, d}$: is the element at position $i, d$ in the tensor $\\mathbf{Z}$.\n",
        "    For a token $x_i$ at the $i$-th position in the sequence, $z_{i, :}$ is the\n",
        "    $D$ dimensional embedding vector for that token.\n",
        "  - $\\mathbf{z}_{i, :}$: is the $D$ dimensional embedding vector for the token\n",
        "    $x_i$ at the $i$-th position in the sequence.\n",
        "\n",
        "    In this context, each token in the sequence is represented by a $D$\n",
        "    dimensional vector. So, the output tensor $\\mathbf{Z}$ captures the dense\n",
        "    representation of the sequence. Each token in the sequence is replaced by\n",
        "    its corresponding embedding vector from the embedding matrix $\\mathbf{E}$.\n",
        "\n",
        "    As before, the output tensor $\\mathbf{Z}$ carries semantic information about\n",
        "    the tokens in the sequence. The closer two vectors are in this embedding\n",
        "    space, the more semantically similar they are.\n",
        "\n",
        "- $\\mathbf{P}$: is the positional encoding tensor, created with sinusoidal\n",
        "  functions of different frequencies:\n",
        "\n",
        "  Each position $i$ in the sequence has a corresponding positional encoding\n",
        "  vector $p_{i, :}$ of length $D$ (the same as the embedding dimension). The\n",
        "  elements of this vector are generated as follows:\n",
        "\n",
        "  $$\n",
        "  p_{i, 2i} = \\sin\\left(\\frac{i}{10000^{2i / D}}\\right)\n",
        "  $$\n",
        "\n",
        "  $$\n",
        "  p_{i, 2i + 1} = \\cos\\left(\\frac{i}{10000^{2i / D}}\\right)\n",
        "  $$\n",
        "\n",
        "  for each $i$ such that $2i < D$ and $2i + 1 < D$.\n",
        "\n",
        "  Thus, the entire tensor $\\mathbf{P}$ is defined as:\n",
        "\n",
        "  $$\n",
        "  \\mathbf{P} = \\begin{bmatrix} p_{1,1} & p_{1,2} & \\cdots & p_{1,D} \\\\ p_{2,1} & p_{2,2} & \\cdots & p_{2,D} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ p_{L,1} & p_{L,2} & \\cdots & p_{L,D} \\end{bmatrix} \\in \\mathbb{R}^{L \\times D}\n",
        "  $$\n",
        "\n",
        "  where\n",
        "\n",
        "  - $L$: is the sequence length.\n",
        "  - $D$: is the embedding dimension.\n",
        "  - $p_{i, d}$: is the element at position $i, d$ in the tensor $\\mathbf{P}$.\n",
        "\n",
        "- Note that $\\mathbf{P}$ is independent of $\\mathbf{Z}$, and it's computed based\n",
        "  on the positional encoding formula used in transformers, which uses sinusoidal\n",
        "  functions of different frequencies:\n",
        "\n",
        "- OVERWRITING $\\mathbf{Z}$: After computing the positional encoding tensor\n",
        "  $\\mathbf{P}$, we can update our original embeddings tensor $\\mathbf{Z}$ to\n",
        "  include positional information:\n",
        "\n",
        "  $$\n",
        "  \\mathbf{Z} = \\mathbf{Z} + \\mathbf{P}\n",
        "  $$\n",
        "\n",
        "  This operation adds the positional encodings to the original embeddings,\n",
        "  giving the final embeddings that are passed to subsequent layers in the\n",
        "  Transformer model.\n",
        "\n",
        "- Or consider using $\\mathbf{Z}^{'}$?\n",
        "\n",
        "### Attention Notations\n",
        "\n",
        "- $H$: Number of attention heads.\n",
        "  - $h$: Index of the attention head.\n",
        "- $d_k = D/H$: Dimension of the keys. In the multi-head attention case, this\n",
        "  would typically be $D/H$ where $D$ is the dimensionality of input embeddings\n",
        "  and $H$ is the number of attention heads.\n",
        "- $d_q = D/H$: Dimension of the queries. Also usually set equal to $d_k$.\n",
        "- $d_v = D/H$: Dimension of the values. Usually set equal to $d_k$.\n",
        "- $\\mathbf{W}^q \\in \\mathbb{R}^{D \\times H \\cdot d_q = D \\times D}$: The query\n",
        "  weight matrix for all heads. It is used to transform the embeddings\n",
        "  $\\mathbf{Z}$ into query representations.\n",
        "\n",
        "- $\\mathbf{W}^k \\in \\mathbb{R}^{D \\times H \\cdot d_k = D \\times D}$: The key\n",
        "  weight matrix for all heads. It is used to transform the embeddings\n",
        "  $\\mathbf{Z}$ into key representations.\n",
        "\n",
        "- $\\mathbf{W}^v \\in \\mathbb{R}^{D \\times H \\cdot d_v = D \\times D}$: The value\n",
        "  weight matrix for all heads. It is used to transform the embeddings\n",
        "  $\\mathbf{Z}$ into value representations.\n",
        "- $\\mathbf{W}_{h}^{q} \\in \\mathbb{R}^{D \\times d_q}$: The query weight matrix\n",
        "  for the $h$-th head. It is used to transform the embeddings $\\mathbf{Z}$ into\n",
        "  query representations for the $h$-th head.\n",
        "  - Important that this matrix collapses to $\\mathbf{W}_{1}^q$ when $H=1$ and\n",
        "    has shape $\\mathbb{R}^{D \\times D}$.\n",
        "  - Note that this weight matrix is derived from $W^q$.\n",
        "- $\\mathbf{W}_{h}^{k} \\in \\mathbb{R}^{D \\times d_k}$: The key weight matrix for\n",
        "  the $h$-th head. It is used to transform the embeddings $\\mathbf{Z}$ into key\n",
        "  representations for the $h$-th head.\n",
        "  - Important that this matrix collapses to $\\mathbf{W}_{1}^k$ when $H=1$ and\n",
        "    has shape $\\mathbb{R}^{D \\times D}$ since $d_k = D/H = D/1 = D$.\n",
        "  - Note that this weight matrix is derived from $W^k$.\n",
        "- $\\mathbf{W}_{h}^{v} \\in \\mathbb{R}^{D \\times d_v}$: The value weight matrix\n",
        "  for the $h$-th head. It is used to transform the embeddings $\\mathbf{Z}$ into\n",
        "  value representations for the $h$-th head.\n",
        "\n",
        "  - Important that this matrix collapses to $\\mathbf{W}_{1}^v$ when $H=1$ and\n",
        "    has shape $\\mathbb{R}^{D \\times D}$.\n",
        "  - Note that this weight matrix is derived from $W^v$.\n",
        "\n",
        "- $\\mathbf{Q} = \\mathbf{Z} \\mathbf{W}^q \\in \\mathbb{R}^{L \\times D}$: The query\n",
        "  matrix. It contains the query representations for all the tokens in the\n",
        "  sequence. This is the matrix that is used to compute the attention scores.\n",
        "  - Each row of the matrix $\\mathbf{Q}$ is a query vector $\\mathbf{q}_{i}$ for\n",
        "    the token at position $i$ in the sequence.\n",
        "- $\\mathbf{Q}_h = \\mathbf{Z} \\mathbf{W}_h^q \\in \\mathbb{R}^{L \\times d_q}$: The\n",
        "  query matrix for the $h$-th head. It contains the query representations for\n",
        "  all the tokens in the sequence. This is the matrix that is used to compute the\n",
        "  attention scores for the $h$-th head.\n",
        "\n",
        "- $\\mathbf{K} = \\mathbf{Z} \\mathbf{W}^k \\in \\mathbb{R}^{L \\times D}$: The key\n",
        "  matrix. It contains the key representations for all the tokens in the\n",
        "  sequence. This is the matrix that is used to compute the attention scores.\n",
        "\n",
        "- $\\mathbf{K}_h = \\mathbf{Z} \\mathbf{W}_h^k \\in \\mathbb{R}^{L \\times d_k}$: The\n",
        "  key matrix for the $h$-th head. It contains the key representations for all\n",
        "  the tokens in the sequence. This is the matrix that is used to compute the\n",
        "  attention scores for the $h$-th head.\n",
        "\n",
        "- $\\mathbf{V} = \\mathbf{Z} \\mathbf{W}^v \\in \\mathbb{R}^{L \\times D}$: The value\n",
        "  matrix. It contains the value representations for all the tokens in the\n",
        "  sequence. This is the matrix where we apply the attention scores to compute\n",
        "  the weighted average of the values.\n",
        "\n",
        "- $\\mathbf{V}_h = \\mathbf{Z} \\mathbf{W}_h^v \\in \\mathbb{R}^{L \\times d_v}$: The\n",
        "  value matrix for the $h$-th head. It contains the value representations for\n",
        "  all the tokens in the sequence. This is the matrix where we apply the\n",
        "  attention scores to compute the weighted average of the values for the $h$-th\n",
        "  head.\n",
        "\n",
        "- $\\mathbf{q}_{i} = \\mathbf{Q}_{i, :} \\in \\mathbb{R}^{d}$: The query vector for\n",
        "  the token at position $i$ in the sequence.\n",
        "- $\\mathbf{k}_{i} = \\mathbf{K}_{i, :} \\in \\mathbb{R}^{d}$: The key vector for\n",
        "  the token at position $i$ in the sequence.\n",
        "- $\\mathbf{v}_{i} = \\mathbf{V}_{i, :} \\in \\mathbb{R}^{d}$: The value vector for\n",
        "  the token at position $i$ in the sequence.\n",
        "- $\\mathbf{A} \\in \\mathbb{R}^{L \\times L}$: The attention matrix. It contains\n",
        "  the attention scores for all the tokens in the sequence. It is computed as:\n",
        "\n",
        "  $$\n",
        "  \\mathbf{A} = \\text{softmax}\\left( \\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_k}} \\right)\n",
        "  $$\n",
        "\n",
        "  where\n",
        "\n",
        "  - $L$: is the sequence length.\n",
        "  - $\\mathbf{Q} \\in \\mathbb{R}^{L \\times D}$: is the query matrix.\n",
        "  - $\\mathbf{K} \\in \\mathbb{R}^{L \\times D}$: is the key matrix.\n",
        "  - $\\sqrt{d_k}$: is the scaling factor.\n",
        "  - $\\text{softmax}(\\cdot)$: is the softmax function applied row-wise.\n",
        "  - More concretely, this is the **self-attention matrix** between an input\n",
        "    sequence $\\mathbf{X} = (x_1, x_2, ..., x_L)$ and itself. Each row in the\n",
        "    matrix $\\mathbf{A}$ is the attention scores for a token in the sequence. The\n",
        "    attention scores are computed by comparing the query vector for a token with\n",
        "    the key vectors for all the tokens in the sequence.\n",
        "  - For instance, if the input sequence is \"cat eat mouse\", then the $L=3$, and\n",
        "    the attention matrix $\\mathbf{A}$'s first row is the attention scores of the\n",
        "    word cat with all other words, (cat & cat, cat & eat, cat & mouse).\n",
        "    Similarly, the second row is the attention scores of the word eat with all\n",
        "    other words, (eat & cat, eat & eat, eat & mouse). Lastly, the third row is\n",
        "    the attention scores of the word mouse with all other words, (mouse & cat,\n",
        "    mouse & eat, mouse & mouse).\n",
        "\n",
        "- $a_{i, j} \\in \\mathbf{A}$: The attention score between the query $i$ and the\n",
        "  key $j$ in the sequence (please do not be confused with the $j$ index in\n",
        "  vocabulary!). It is computed as:\n",
        "\n",
        "  $$\n",
        "  a_{i, j} = \\text{softmax}\\left(\\frac{\\mathbf{q}_{i} \\mathbf{k}_{j}^T}{\\sqrt{d_k}}\\right)\n",
        "  $$\n",
        "\n",
        "  where\n",
        "\n",
        "  - $\\mathbf{q}_{i} \\in \\mathbb{R}^{d}$: is the query vector for the $i$-th\n",
        "    token in the sequence.\n",
        "  - $\\mathbf{k}_{j} \\in \\mathbb{R}^{d}$: is the key vector for the $j$-th token\n",
        "    in the sequence.\n",
        "\n",
        "- $f(\\cdot)$: Attention function (such as additive attention or scaled\n",
        "  dot-product attention).\n",
        "\n",
        "  - Should we find a better notation?\n",
        "\n",
        "    The scaled dot-product attention function $f(\\cdot)$ can be formulated as:\n",
        "\n",
        "    $$\n",
        "    \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) := f(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left( \\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_k}} \\right) \\mathbf{V} \\in \\mathbb{R}^{L \\times D}\n",
        "    $$\n",
        "\n",
        "    or you can also substitute $\\mathbf{A}$ to get the same result:\n",
        "\n",
        "    $$\n",
        "    \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) := f(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\mathbf{A} \\mathbf{V}\n",
        "    $$\n",
        "\n",
        "    For the $h$-th head, it can be represented as:\n",
        "\n",
        "    $$\n",
        "    f(\\mathbf{Q}_h, \\mathbf{K}_h, \\mathbf{V}_h) = \\text{softmax}\\left( \\frac{\\mathbf{Q}_h \\mathbf{K}_h^T}{\\sqrt{d_k}} \\right) \\mathbf{V}_h\n",
        "    $$\n",
        "\n",
        "    In these formulas, $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$ are the\n",
        "    query, key, and value matrices, respectively. The function\n",
        "    $\\text{softmax}(\\cdot)$ is applied row-wise. The division by $\\sqrt{d_k}$ is\n",
        "    a scaling factor that helps in training stability.\n",
        "\n",
        "---\n",
        "\n",
        "- $\\mathbf{h}_i \\in \\mathbb{R}^{p_v}$: Output of the $i$-th attention head.\n",
        "\n",
        "- $\\mathbf{W}_o \\in \\mathbb{R}^{p_o \\times h p_v}$: Output weight matrix, used\n",
        "  to transform the concatenation of all head outputs.\n",
        "\n",
        "- $p_o$: Dimension of the final output after applying the output weight matrix\n",
        "  $\\mathbf{W}_o$.\n",
        "\n",
        "Let's break this down:\n",
        "\n",
        "- $\\mathbf{h}_i \\in \\mathbb{R}^{p_v}$: Output of the $i$-th attention head. It\n",
        "  is computed as a function $f$ which applies attention (such as additive\n",
        "  attention or scaled dot-product attention) to the transformed queries, keys\n",
        "  and values. This function depends on the query $\\mathbf{q}$, key $\\mathbf{k}$,\n",
        "  and value $\\mathbf{v}$, and the weight matrices $\\mathbf{W}_i^{(q)}$,\n",
        "  $\\mathbf{W}_i^{(k)}$, and $\\mathbf{W}_i^{(v)}$. The dimensions $p_q$, $p_k$,\n",
        "  and $p_v$ denote the output dimensions of the query, key and value\n",
        "  transformations respectively, for the $i$-th head.\n",
        "\n",
        "- $\\mathbf{W}_i^{(q)} \\in \\mathbb{R}^{p_q \\times d_q}$,\n",
        "  $\\mathbf{W}_i^{(k)} \\in \\mathbb{R}^{p_k \\times d_k}$, and\n",
        "  $\\mathbf{W}_i^{(v)} \\in \\mathbb{R}^{p_v \\times d_v}$: The weight matrices for\n",
        "  the $i$-th attention head. These are used to transform the query, key, and\n",
        "  value inputs to the dimensions suitable for the attention mechanism.\n",
        "\n",
        "- $f(\\cdot)$: This function represents the attention mechanism (like additive\n",
        "  attention or scaled dot-product attention). It takes as input the transformed\n",
        "  query, key, and value vectors and produces the output of the attention head.\n",
        "\n",
        "- $\\mathbf{W}_o \\in \\mathbb{R}^{p_o \\times h p_v}$: This is the output weight\n",
        "  matrix that linearly transforms the concatenation of the outputs from all\n",
        "  attention heads to produce the final output of the multi-head attention\n",
        "  mechanism.\n",
        "\n",
        "- The expression\n",
        "  $\\mathbf{W}_o\\left[\\begin{array}{c}\n",
        "\\mathbf{h}_1 \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{h}_h\n",
        "\\end{array}\\right] \\in \\mathbb{R}^{p_o}$\n",
        "  represents the final output of the multi-head attention layer. It's the result\n",
        "  of applying the linear transformation defined by $\\mathbf{W}_o$ to the\n",
        "  concatenated outputs of all attention heads.\n",
        "\n",
        "This notation helps us understand the inner workings of the multi-head attention\n",
        "mechanism, and it provides a clear path for implementing the multi-head\n",
        "attention mechanism in a neural network model.\n",
        "\n",
        "## Attention (Show this first)\n",
        "\n",
        "Both `torch.bmm` and `torch.matmul` can be used for matrix multiplication in\n",
        "PyTorch, but their use cases and behaviors are somewhat different, especially\n",
        "with higher-dimensional tensors. Let's break this down:\n",
        "\n",
        "1. **torch.bmm**:\n",
        "\n",
        "   - It stands for \"batch matrix multiplication\".\n",
        "   - It expects tensors to be of rank 3: `(batch_size, rows, cols)`.\n",
        "   - It performs matrix multiplication for each batch between corresponding\n",
        "     matrices.\n",
        "\n",
        "   For example, given two tensors `A` of shape `(B, M, N)` and `B` of shape\n",
        "   `(B, N, P)`, the output will be of shape `(B, M, P)`.\n",
        "\n",
        "2. **torch.matmul**:\n",
        "\n",
        "   - It's a more general-purpose matrix multiplication function.\n",
        "   - When two 3D tensors are passed, it behaves like `torch.bmm`.\n",
        "   - However, it can handle tensors of rank > 3 as well. When given\n",
        "     higher-dimensional tensors, it considers the last two dimensions as\n",
        "     matrices to be multiplied and broadcasts over the remaining dimensions.\n",
        "\n",
        "   Given two tensors `A` of shape `(X, Y, M, N)` and `B` of shape\n",
        "   `(X, Y, N, P)`, the output will be of shape `(X, Y, M, P)`.\n",
        "\n",
        "In the context of the provided code, both methods achieve the same result\n",
        "because:\n",
        "\n",
        "- The shape of the `queries` and transposed `keys` tensors matches the expected\n",
        "  input shape for `torch.bmm` in the `DotProductAttention` class.\n",
        "- In the `attention` function, the shape of `query` and transposed `key` tensors\n",
        "  is also compatible with both `torch.bmm` and `torch.matmul`.\n",
        "\n",
        "So, when used for 3D tensors, `torch.bmm` and `torch.matmul` can give the same\n",
        "result. The discrepancy arises primarily with higher-dimensional tensors, where\n",
        "the broadcasting behavior of `torch.matmul` distinguishes it from `torch.bmm`.\n",
        "\n",
        "## How $W^{q}_i$ is implemented in practice?\n",
        "\n",
        "The notation $W^{q}_i$ is used in the paper to denote the weight matrix for the\n",
        "queries (Q) of the $i$-th head. However, it's essential to understand how this\n",
        "is implemented in practice.\n",
        "\n",
        "The entire process can be seen as a two-step operation:\n",
        "\n",
        "1. **Apply Linear Transformations**: You apply linear transformations to the\n",
        "   whole embeddings to create larger matrices for Q, K, V. These matrices have\n",
        "   dimensions that account for all heads. In practice, this can be implemented\n",
        "   using a single linear layer, such as:\n",
        "\n",
        "   $$\n",
        "   Q = \\text{{embeddings}} @ \\mathbf{W}^q\n",
        "   $$\n",
        "\n",
        "   where $\\mathbf{W}^q$ has dimensions $D \\times (h \\cdot d_q)$.\n",
        "\n",
        "2. **Reshape and Split**: After applying the linear transformations, you reshape\n",
        "   and split the result into individual heads. The reshaping ensures that the\n",
        "   final dimensions are $[N, H, S, d_q]$, where $N$ is the batch size, $H$ is\n",
        "   the number of heads, $S$ is the sequence length, and $d_q$ is the dimension\n",
        "   of queries per head.\n",
        "\n",
        "So, while the paper uses notation like $W^{q}_i$, this doesn't mean that you\n",
        "directly apply a different linear transformation to different parts of the\n",
        "embeddings. Instead, you apply a single large linear transformation to the whole\n",
        "embeddings and then reshape the result to obtain the individual heads.\n",
        "\n",
        "In mathematical terms, the overall operation can be seen as:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "Q_{\\text{{all heads}}} & = \\text{{embeddings}} @ \\mathbf{W}^q \\\\\n",
        "Q_{\\text{{head i}}} & = Q_{\\text{{all heads}}}[:, i \\cdot d_q : (i + 1) \\cdot d_q]\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Here, $Q_{\\text{{all heads}}}$ is the result of applying the linear\n",
        "transformation, and $Q_{\\text{{head i}}}$ is the portion corresponding to the\n",
        "$i$-th head, obtained by slicing along the last dimension.\n",
        "\n",
        "## HEad is similar to kernels in CNN\n",
        "\n",
        "The multi-head attention mechanism is similar to the convolutional layer in\n",
        "convolutional neural networks. In a convolutional layer, you apply multiple\n",
        "kernels to the input to obtain multiple feature maps. Similarly, in the\n",
        "multi-head attention mechanism, you apply multiple attention heads to the input\n",
        "to obtain multiple output vectors.\n",
        "\n",
        "## so the catch is you do not split the embeddings in H heads, instead you split the linear transformed embeddings?\n",
        "\n",
        "```\n",
        "# Apply linear transformations to compute Q, K, V\n",
        "# NOTE: here is an important misconception that if you have\n",
        "# 8 heads, then you SPLIT the embeddings into 8 parts and\n",
        "# then apply linear transformations to each part. This is\n",
        "# WRONG. You apply linear transformations to the whole\n",
        "# embeddings and then split the result into 8 parts.\n",
        "```\n",
        "\n",
        "You don't split the original embeddings into $H$ heads; instead, you apply\n",
        "linear transformations to the original embeddings and then split the transformed\n",
        "embeddings into $H$ heads.\n",
        "\n",
        "Here's the step-by-step process again, highlighting this specific aspect:\n",
        "\n",
        "1. Apply linear transformations for queries, keys, and values to the entire\n",
        "   embeddings, creating matrices $Q, K,$ and $V$.\n",
        "2. Split these transformed matrices into $H$ different heads, each having lower\n",
        "   dimensions (e.g., if the original dimension is 512 and there are 8 heads,\n",
        "   each head will have a dimension of 64).\n",
        "3. Process each head through the Scaled Dot-Product Attention mechanism.\n",
        "4. Concatenate the outputs from all the heads and pass through a final linear\n",
        "   layer.\n",
        "\n",
        "The split after the linear transformations allows the model to create multiple\n",
        "different projections of the input and process them independently. This enables\n",
        "the model to focus on different aspects of the input across the different heads,\n",
        "enhancing its ability to model complex relationships.\n",
        "\n",
        "## Confusion on Weight matrix per head\n",
        "\n",
        "The notation and explanation in the original papers and many articles do indeed\n",
        "mention separate weight matrices for each head, such as $W^{Q}_i$, but in\n",
        "implementation, it's common to represent these separate weights within a single\n",
        "large weight matrix. The notation might be different, but the mathematical\n",
        "operation is equivalent.\n",
        "\n",
        "Here's how the two approaches relate:\n",
        "\n",
        "1. **Separate Weight Matrices Notation (Paper Notation):** In the theoretical\n",
        "   description, you can imagine having separate weight matrices $W^{Q}_i$ for\n",
        "   each head, and then you multiply the input embeddings by each weight matrix,\n",
        "   applying the transformation separately for each head.\n",
        "\n",
        "2. **Single Large Weight Matrix Implementation (Your Code):** In your\n",
        "   implementation, you create one large weight matrix, $W_q$, that combines all\n",
        "   the individual weight matrices for each head. When you multiply the input\n",
        "   embeddings by $W_q$, you create a large transformed matrix. Then, by slicing\n",
        "   this large matrix, you separate it into $H$ different heads, effectively\n",
        "   applying the individual weight matrices $W^{Q}_i$ for each head.\n",
        "\n",
        "The two approaches are mathematically equivalent. In the second approach, the\n",
        "separate weight matrices for each head are not explicitly defined as learnable\n",
        "parameters in the code. Instead, they are implicitly represented within the\n",
        "single large weight matrix $W_q$ and separated by slicing after the linear\n",
        "transformation.\n",
        "\n",
        "This approach can be more efficient computationally and often aligns better with\n",
        "hardware and library optimizations, but it may create confusion when comparing\n",
        "to the paper's notation. The key is to understand that the mathematical\n",
        "relationships and learning dynamics are the same, even though the notation and\n",
        "coding structure might differ.\n",
        "\n",
        "### Approach 1: Single Large Weight Matrix Implementation (Paper's Code)\n",
        "\n",
        "In this approach, we concatenate all the individual weight matrices $W^{q}_{h}\n",
        "$\n",
        "into one large weight matrix $W_q$:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "W_q =  \\begin{bmatrix} w^{q}_{1,1} & w^{q}_{1,2} & \\ldots & w^{q}_{1,D} \\\\\n",
        "w^{q}_{2,1} & w^{q}_{2,2} & \\ldots & w^{q}_{2,D} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "w^{q}_{D,1} & w^{q}_{D,2} & \\ldots & w^{q}_{D,D} \\end{bmatrix}_{D \\times D}\n",
        "&= \\begin{bmatrix} W^{q}_1 & W^{q}_2 & \\ldots & W^{q}_H \\end{bmatrix} \\in \\mathbb{R}^{D \\times D}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where each $W^{q}_h$ is a matrix with dimensions\n",
        "$D \\times \\frac{D}{H} = D \\times d_q$.\n",
        "\n",
        "In other words, if the embedding dimension is 512 and there are 8 heads, the\n",
        "original $W^q$ matrix is of size $512 \\times 512$, and we can decompose it into\n",
        "8 matrices of size $512 \\times 64$, each forming a column of the original\n",
        "matrix.\n",
        "\n",
        "---\n",
        "\n",
        "Side note: if users wanna see jacobian like block matrics:\n",
        "\n",
        "We can represent the matrix in blocks by grouping its elements. Here's an\n",
        "example that might suit your purpose:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "W_q &=  \\begin{bmatrix}\n",
        "B^{q}_{1,1} & B^{q}_{1,2} & \\ldots & B^{q}_{1,H} \\\\\n",
        "B^{q}_{2,1} & B^{q}_{2,2} & \\ldots & B^{q}_{2,H} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "B^{q}_{H,1} & B^{q}_{H,2} & \\ldots & B^{q}_{H,H} \\\\\n",
        "\\end{bmatrix}_{D \\times D}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Where each block $B^{q}_{i,j}$ is a sub-matrix of size $m \\times m$ (assuming\n",
        "$D$ is divisible by $H$ and $m = \\frac{D}{H}$) and can be represented as:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "B^{q}_{i,j} =  \\begin{bmatrix}\n",
        "w^{q}_{i \\cdot m - m + 1, j \\cdot m - m + 1} & \\ldots & w^{q}_{i \\cdot m - m + 1, j \\cdot m} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "w^{q}_{i \\cdot m, j \\cdot m - m + 1} & \\ldots & w^{q}_{i \\cdot m, j \\cdot m}\n",
        "\\end{bmatrix}_{m \\times m}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "This representation can help visualize the matrix as a composition of smaller\n",
        "blocks, which might be useful in certain contexts, such as when dealing with\n",
        "partitioned matrices in numerical computations.\n",
        "\n",
        "---\n",
        "\n",
        "We then multiply the embeddings by this large weight matrix:\n",
        "\n",
        "$$\n",
        "Q = \\mathbf{Z} \\cdot W^{q}\n",
        "$$\n",
        "\n",
        "Then, we slice the result into $H$ parts:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "Q_{h} \\in \\mathbb{R}^{B \\times L \\times d_q} &= Q\\left[:, :, h \\cdot \\frac{D}{H} : (h+1) \\cdot \\frac{D}{H}\\right] \\\\\n",
        "&= Q\\left[:, :, h \\cdot d_q : (h+1) \\cdot d_q\\right] \\\\\n",
        "&= \\mathbf{Z} \\cdot W^{q}_{h}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $W^{q}_{h}$ is the submatrix of $W^{q}$ that corresponds to the $h$-th\n",
        "head, or in other words, let's say $W^{q}_1$, the first head, it means\n",
        "subsetting the $W^q$ with rows dimension unchanged (i.e. 512), and taking the\n",
        "first 64 columns, resulting in a matrix of size $512 \\times 64$.\n",
        "\n",
        "### Approach 2: Separate Weight Matrices Notation (Paper Notation)\n",
        "\n",
        "Suppose we have $H$ heads and our embedding matrix $\\mathbf{Z}$ has dimensions\n",
        "$B \\times L \\times D$, where $B$ is the batch size, $L$ is the sequence length,\n",
        "and $D$ is the embedding dimension.\n",
        "\n",
        "For each head $h$, we have a weight matrix $W^{q}_{h}$ with dimensions\n",
        "$D\n",
        "\\times \\frac{D}{H} = D \\times d_q$, and we apply this transformation to the\n",
        "embeddings:\n",
        "\n",
        "$$\n",
        "Q_{h} \\in \\mathbb{R}^{B \\times L \\times d_q} = \\mathbf{Z} \\cdot W^{q}_{h}\n",
        "$$\n",
        "\n",
        "So the confusion arises because in the code implementation we do not see an\n",
        "explicit definition of the separate weight matrices $W^{q}_{h}$, but they are\n",
        "implicitly represented within the single large weight matrix $W^{q}$. But\n",
        "actually you can see from approach 1, the $W^q$ is just a concatenation of all\n",
        "the $W^{q}_{h}$, so it's just a different way of representing the same thing.\n",
        "\n",
        "## is the FFN in encoder just a MLP layer\n",
        "\n",
        "Yes, the Feed-Forward Network (FFN) in the Transformer's encoder is essentially\n",
        "a Multi-Layer Perceptron (MLP) layer. It typically consists of two fully\n",
        "connected layers, with a non-linear activation function (usually ReLU) applied\n",
        "after the first layer.\n",
        "\n",
        "Here's the general structure of the FFN in the Transformer's encoder:\n",
        "\n",
        "1. **First Linear Layer:** The input is passed through a fully connected linear\n",
        "   layer with weight matrix $W_1$ and bias $b_1$.\n",
        "2. **Activation Function:** A non-linear activation function (such as ReLU) is\n",
        "   applied to the result of the first linear layer.\n",
        "3. **Second Linear Layer:** The activated output is then passed through another\n",
        "   fully connected linear layer with weight matrix $W_2$ and bias $b_2$.\n",
        "4. **Optional Dropout:** Some implementations might include dropout for\n",
        "   regularization after one or both of the linear layers.\n",
        "\n",
        "The mathematical expression for this process would look something like:\n",
        "\n",
        "$$\n",
        "\\text{FFN}(x) = W_2 \\cdot \\text{ReLU}(W_1 \\cdot x + b_1) + b_2\n",
        "$$\n",
        "\n",
        "Where $x$ is the input to the FFN, and $W_1$, $W_2$, $b_1$, and $b_2$ are\n",
        "learnable parameters.\n",
        "\n",
        "So, the FFN in the Transformer's encoder is effectively a specific form of a\n",
        "Multi-Layer Perceptron with two layers, with the goal of learning position-wise\n",
        "transformations of the input.\n",
        "\n",
        "## All the Whys?\n",
        "\n",
        "<https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html>\n",
        "\n",
        "### What are the different Subspaces?\n",
        "\n",
        "See <https://www.youtube.com/watch?v=UPtG_38Oq8o> around 25min...\n",
        "\n",
        "Change X to Z.\n",
        "\n",
        "To explain the idea that the keys and queries matrices act as linear\n",
        "transformations to enhance embeddings for attention, let's break it down\n",
        "step-by-step.\n",
        "\n",
        "1. **Background Context**: When working with the transformer architecture, or\n",
        "   any architecture that utilizes the attention mechanism, we begin with input\n",
        "   embeddings. These embeddings are vectors that represent the information\n",
        "   (usually words or subwords) that we want the model to process.\n",
        "\n",
        "1. **Assumptions**:\n",
        "\n",
        "- Let's assume we have a sequence of tokens, represented by the embedding matrix\n",
        "  $\\mathbf{X} \\in \\mathbb{R}^{L \\times D}$, where $L$ is the sequence length and\n",
        "  $D$ is the embedding dimension.\n",
        "- We want to obtain the query, key, and value matrices from these embeddings.\n",
        "  Each of these matrices is produced by multiplying the embeddings with their\n",
        "  respective weight matrices: $\\mathbf{W^Q}$, $\\mathbf{W^K}$, and\n",
        "  $\\mathbf{W^V}$.\n",
        "\n",
        "1. **Step-by-Step Transformation**:\n",
        "\n",
        "- **Query Matrix**: To obtain the query matrix $\\mathbf{Q}$, we perform the\n",
        "  following linear transformation:\n",
        "\n",
        "  $$\n",
        "  \\mathbf{Q} = \\mathbf{X} \\mathbf{W^Q}\n",
        "  $$\n",
        "\n",
        "  Here, $\\mathbf{W^Q} \\in \\mathbb{R}^{D \\times D'}$ is the weight matrix for the\n",
        "  queries, and $D'$ is the dimensionality of the query vectors. This\n",
        "  multiplication transforms the embeddings in $\\mathbf{X}$ to enhance them for\n",
        "  the attention mechanism's querying process.\n",
        "\n",
        "- **Key Matrix**: Similarly, to obtain the key matrix $\\mathbf{K}$, we do:\n",
        "\n",
        "  $$\n",
        "  \\mathbf{K} = \\mathbf{X} \\mathbf{W^K}\n",
        "  $$\n",
        "\n",
        "  Here, $\\mathbf{W^K} \\in \\mathbb{R}^{D \\times D'}$ is the weight matrix for the\n",
        "  keys.\n",
        "\n",
        "- **Value Matrix**: The value matrix $\\mathbf{V}$ is obtained in a similar\n",
        "  manner:\n",
        "  $$\n",
        "  \\mathbf{V} = \\mathbf{X} \\mathbf{W^V}\n",
        "  $$\n",
        "  Here, $\\mathbf{W^V} \\in \\mathbb{R}^{D \\times D'}$ is the weight matrix for the\n",
        "  values.\n",
        "\n",
        "4. **Reasoning**: These linear transformations project the original embeddings\n",
        "   into a space where the attention mechanism can more effectively compute\n",
        "   similarities (for queries and keys) and aggregate information (for values).\n",
        "   The weight matrices $\\mathbf{W^Q}$, $\\mathbf{W^K}$, and $\\mathbf{W^V}$ are\n",
        "   learned during training to optimize the attention mechanism's performance for\n",
        "   the given task.\n",
        "\n",
        "In summary, by applying these transformations, the model can focus on different\n",
        "aspects of the input data when computing attention scores and aggregating\n",
        "information, leading to a more powerful and flexible representation.\n",
        "\n",
        "### Why Softmax?\n",
        "\n",
        "See <https://www.youtube.com/watch?v=UPtG_38Oq8o> around 16 min.\n",
        "\n",
        "### Why Scaling by $\\sqrt{d_k}$?\n",
        "\n",
        "- <https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html>\n",
        "- <https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html>\n",
        "\n",
        "### Why need Positional Encoding?\n",
        "\n",
        "## References and Further Readings\n",
        "\n",
        "1. Stanford CS224N Course: [link](https://web.stanford.edu/class/cs224n/)\n",
        "2. Aman AI Transformers Primer:\n",
        "   [link](https://aman.ai/primers/ai/transformers/#transformer-core)\n",
        "3. Implementing a Transformer from Scratch in PyTorch:\n",
        "   [link](https://www.lesswrong.com/posts/2kyzD5NddfZZ8iuA7/implementing-a-transformer-from-scratch-in-pytorch-a-write)\n",
        "4. Illustrated Transformer by Jay Alammar:\n",
        "   [link](http://jalammar.github.io/illustrated-transformer/)\n",
        "5. Mislav Juric's Transformer from Scratch:\n",
        "   [link](https://github.com/MislavJuric/transformer-from-scratch/blob/main/layers/MultiHeadAttention.py)\n",
        "6. Peter Bloem's Blog on Transformers:\n",
        "   [link](https://peterbloem.nl/blog/transformers)\n",
        "7. Transformer Family Explained by Lilian Weng:\n",
        "   [link](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)\n",
        "8. D2L AI Book - Multihead Attention:\n",
        "   [link](https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html)\n",
        "9. NLP Course at NTU:\n",
        "   [link](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/self_v7.pdf)\n",
        "10. Attention Is All You Need (Original Transformer Paper):\n",
        "    [link](https://arxiv.org/pdf/1706.03762.pdf)\n",
        "11. Harvard NLP - Attention in Transformers:\n",
        "    [link](https://nlp.seas.harvard.edu/2018/04/03/attention.html#batches-and-masking)\n",
        "12. Annotated Transformer:\n",
        "    [link](http://nlp.seas.harvard.edu/annotated-transformer/)\n",
        "13. LabML AI - MultiHead Attention:\n",
        "    [link](https://nn.labml.ai/transformers/mha.html)\n",
        "14. NTU Speech and Language Processing Course:\n",
        "    [link](https://speech.ee.ntu.edu.tw/~hylee/ml/2023-spring.php)\n",
        "15. Google Colab - Self-Attention Example:\n",
        "    [link](https://colab.research.google.com/drive/1u-610KA-urqfJjDH5O0pecwfP--V9DQs?usp=sharing#scrollTo=iXZ5B0EKJGs8)\n",
        "16. Self-Attention from Scratch by Sebastian Raschka:\n",
        "    [link](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)\n",
        "17. UvA DL Course - Transformers and MHAttention:\n",
        "    [link](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)\n",
        "18. Simple Attention-based Text Prediction Model:\n",
        "    [link](https://datascience.stackexchange.com/questions/94205/a-simple-attention-based-text-prediction-model-from-scratch-using-pytorch)\n",
        "19. The AI Summer - Self-Attention Explanation:\n",
        "    [link](https://theaisummer.com/self-attention/#how-multi-head-attention-works-in-detail)\n"
      ],
      "metadata": {
        "id": "8PszNdc_E5D9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common Questions\n",
        "\n",
        "## Understanding Embedding Searches: Beyond Cosine Similarity\n",
        "\n",
        "It is a common misconception that search operations among embeddings, which\n",
        "often employ metrics such as cosine similarity, are of limited utility because\n",
        "sentences can vary widely in context and meaning. However, I believe this\n",
        "viewpoint overlooks the capabilities of attention-based models. Such models are\n",
        "adept at generating contextually relevant embeddings. By mapping both the query\n",
        "and document embeddings into the same D-dimensional space, the effectiveness of\n",
        "distance-based metrics is preserved for search tasks. This is because the\n",
        "embeddings are now infused with rich semantic and contextual information, thus\n",
        "enabling a more nuanced and accurate retrieval of results.\n",
        "\n",
        "Why?\n",
        "\n",
        "**Cosine Similarity and Contextual Differences**: It's true that cosine\n",
        "similarity measures the cosine of the angle between two vectors, which in itself\n",
        "doesn't account for the complexity of language. This can be a limitation when\n",
        "using simpler methods of vectorization like bag-of-words or TF-IDF, where the\n",
        "vectors may not accurately represent the meaning of the text due to their\n",
        "inability to capture context and semantics.\n",
        "\n",
        "**Attention Mechanisms**: Attention-based models, like those found in BERT or\n",
        "GPT, fundamentally change this dynamic. They use self-attention mechanisms to\n",
        "generate embeddings of text that consider the entire context in which each word\n",
        "appears. This means that the word \"bank\" would have a different embedding when\n",
        "used in the context of a river than when used in the context of a financial\n",
        "institution, even though the cosine similarity might be used to compare these\n",
        "embeddings.\n",
        "\n",
        "**Contextual Embeddings in High-Dimensional Space**: When we talk about\n",
        "projecting queries and documents into the same D-dimensional space, what we are\n",
        "referring to is a vector space where the dimensions capture semantic and\n",
        "syntactic aspects of language as learned by the model during training on a\n",
        "diverse corpus. In this space, distance metrics like cosine similarity become\n",
        "powerful because the vectors themselves are encoded with rich contextual\n",
        "information.\n",
        "\n",
        "**Semantic and Contextual Relevance**: With attention-based models, the\n",
        "embedding for a sentence or a document isn't just a sum of its word vectors.\n",
        "Instead, it's a complex aggregation that takes into account the sentence\n",
        "structure, word interactions, and overall meaning. This makes the embeddings\n",
        "contextually aware, so even if two sentences are quite different in terms of\n",
        "their surface-level word content, if they share the same underlying meaning or\n",
        "are contextually related, their embeddings will be close in the high-dimensional\n",
        "space, and cosine similarity can effectively capture this closeness.\n",
        "\n",
        "Therefore, when using embeddings from attention-based models, cosine similarity\n",
        "(or other distance-based metrics) for retrieving documents based on a query\n",
        "becomes a powerful tool because it's leveraging these rich, contextually\n",
        "informed representations of the text. This capability is at the core of why\n",
        "models like BERT have been revolutionary for tasks that involve understanding\n",
        "the semantic similarity between pieces of text, such as information retrieval,\n",
        "question answering, and document classification.\n"
      ],
      "metadata": {
        "id": "rNYvgad6MXIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Kte9zVp7M6hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Walkthrough\n",
        "\n",
        "##\n",
        "\n",
        "Let's use the sentence \"The cat walks by the bank\" to walk through the\n",
        "self-attention mechanism with analogies and to clarify how it works step by\n",
        "step.\n",
        "\n",
        "**Setting the Scene (Embedding the Sentence):** Imagine each word in the\n",
        "sentence is a person at a party (our tokens). They start by telling a basic fact\n",
        "about themselves (their initial embedding).\n",
        "\n",
        "**The Roles:**\n",
        "\n",
        "- **Q (Seekers)**: Each person (word) is curious about the stories (contexts) of\n",
        "  others at the party. They have their own perspective or question (Q vector).\n",
        "- **K (Holders)**: At the same time, each person has a name tag with keywords\n",
        "  that describe their story (K vector).\n",
        "- **V (Retrievers)**: They also hold a bag of their experiences (V vector),\n",
        "  ready to share.\n",
        "\n",
        "**Transformations (Applying W Matrices):** We give each person a set of glasses\n",
        "(the matrices $W_Q, W_K, W_V$) that changes how they see the world (the space\n",
        "they project to).\n",
        "\n",
        "- With $W_Q$ glasses, they focus on what they want to know from others.\n",
        "- With $W_K$ glasses, they highlight their name tag details, making some\n",
        "  features stand out more.\n",
        "- With $W_V$ glasses, they prepare to share the contents of their bag\n",
        "  effectively.\n",
        "\n",
        "**Attention (Calculating Q @ K.T):** Now, each person looks around the room\n",
        "(sequence) with their $W_Q$ glasses and sees the highlighted name tags (after\n",
        "$W_K$ transformation) of everyone else. They measure how similar their question\n",
        "is to the others' name tags—this is the dot product $Q @ K^T$.\n",
        "\n",
        "For \"cat,\" let’s say it’s curious about the notion of \"walking\" and \"bank.\" It\n",
        "will measure the similarity (attention scores) between its curiosity and the\n",
        "name tags of \"walks,\" \"by,\" \"the,\" \"bank.\"\n",
        "\n",
        "**Normalization (Softmax):** After measuring, \"cat\" decides how much to focus on\n",
        "each story—this is softmax. Some stories are very relevant (\"walks\"), some\n",
        "moderately (\"by,\" \"the\"), and some might be highly relevant depending on context\n",
        "(\"bank\" — is it a river bank or a financial institution?).\n",
        "\n",
        "**Retrieval (Applying Attention to V):** Now \"cat\" decides to listen to the\n",
        "stories in proportion to its focus. It takes pieces (weighted by attention\n",
        "scores) from each person's experience bag (V vectors) and combines them into a\n",
        "richer, contextual understanding of itself in the sentence. This combination\n",
        "gives us the new representation of \"cat,\" informed by the entire context of the\n",
        "sentence.\n",
        "\n",
        "In essence:\n",
        "\n",
        "- **Q (Query):** What does \"cat\" want to know?\n",
        "- **K (Key):** Who has relevant information to \"cat\"’s curiosity?\n",
        "- **V (Value):** What stories does \"cat\" gather from others, and how much does\n",
        "  it take from each to understand its role in the sentence?\n",
        "\n",
        "The output of self-attention for \"cat\" now encapsulates not just \"cat\" but its\n",
        "relationship and relevance to \"walks,\" \"by,\" \"the,\" \"bank\" in a way that no\n",
        "single word could convey alone. This output then becomes the input to the next\n",
        "layer, where the process can repeat, enabling the model to develop an even more\n",
        "nuanced understanding.\n"
      ],
      "metadata": {
        "id": "nVjROr0RNAAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Comments\n",
        "\n",
        "## Positional Encodings\n",
        "\n",
        "### Why do we hardcode batch size of 1 when creating P?\n",
        "\n",
        "The tensor $P$ for positional encoding is initialized with a batch size of 1.\n",
        "This makes it easy to add to the actual input sequences later, during the\n",
        "forward pass. Positional encodings are not dependent on the specific input\n",
        "sequence but are a function of the position within the sequence. Therefore,\n",
        "they can be precomputed and stored.\n",
        "When you look at the forward pass:\n",
        "\n",
        "```python\n",
        "def forward(self, Z: torch.Tensor) -> torch.Tensor:\n",
        "    Z = self._add_positional_encoding(Z)\n",
        "    return self.dropout(Z)\n",
        "```\n",
        "\n",
        "and the `_add_positional_encoding` method:\n",
        "\n",
        "```python\n",
        "def _add_positional_encoding(self, Z: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Add the positional encoding tensor to the input tensor.\"\"\"\n",
        "    return Z + self.P[:, : Z.shape[1], :].to(Z.device)\n",
        "```\n",
        "\n",
        "You'll see that $P$ is sliced to match the sequence length of $Z$ and\n",
        "then added to $Z$. Because of broadcasting rules in PyTorch,\n",
        "$P$ will automatically be broadcasted to the batch size of $Z$\n",
        "during this addition. This is why $P$ is initialized with a batch size of 1;\n",
        "it keeps the implementation flexible while making the broadcasting implicit.\n",
        "\n",
        "### Why do we register P as a buffer in PyTorch?\n",
        "\n",
        "In your `PositionalEncoding` class, the tensor `self.P` holds the pre-computed positional encodings. If you intend for this tensor to be automatically moved to the correct device when the module is moved, and if it should not be a learnable parameter, then registering it as a buffer would be a good idea. This ensures that `self.P` is part of the module's state but is not updated during backpropagation.\n",
        "\n",
        "You could register `self.P` as a buffer right after you initialize it in the `_init_positional_encoding` method:\n",
        "\n",
        "```python\n",
        "def _init_positional_encoding(self) -> torch.Tensor:\n",
        "    \"\"\"Initialize the positional encoding tensor.\"\"\"\n",
        "    P = torch.zeros((1, self.max_len, self.d_model))\n",
        "    position = self._get_position_vector()\n",
        "    div_term = self._get_div_term_vector()\n",
        "    P[:, :, 0::2] = torch.sin(position / div_term)\n",
        "    P[:, :, 1::2] = torch.cos(position / div_term)\n",
        "    self.register_buffer(\"P\", P, persistent=True)\n",
        "    return P\n",
        "```\n",
        "\n",
        "Using `register_buffer` ensures that:\n",
        "\n",
        "1. `self.P` is automatically moved to the device the model is moved to (e.g., from CPU to GPU).\n",
        "2. `self.P` is saved when you save the model using `torch.save` or `torch.load`.\n",
        "\n",
        "The `persistent=False` argument indicates that the buffer should not be part of the model's `state_dict`, meaning it won't be saved or loaded with the model. If you do want it to be part of the `state_dict`, you can simply omit this argument.\n",
        "\n",
        "## Attention\n",
        "\n",
        "### Why do we call contiguous on Q, K and V?\n",
        "\n",
        "D2L's code uses `reshape` to reshape the `Q`, `K` and `V`, where\n",
        "other code such as from the Annotated Transformer uses `view`.\n",
        "When you use `view`, this assumes the tensor is `contiguous`,\n",
        "so it is better to call `contiguous` first.\n",
        "\n",
        "### Why do we want to transpose Q, K, and V?\n",
        "\n",
        "The transposition of $Q$, $K$, and $V$ in multi-head attention serves a specific purpose: to allow for parallel computation across multiple attention heads. In the original shape, the \"heads\" dimension does not exist; the tensor is simply $B \\times L \\times D$, where $B$ is the batch size, $L$ is the sequence length, and $D$ is the model dimension. By transposing, we create a new shape $B \\times H \\times L \\times (D/H)$, where $H$ is the number of heads. This enables the following:\n",
        "\n",
        "1. **Parallelization**: Each head can now be computed in parallel since each head operates independently of the others.\n",
        "2. **Optimization**: Modern hardware accelerators like GPUs are optimized for certain tensor operations, and having a shape that aligns well with these optimizations can result in faster computation.\n",
        "3. **Readability and Maintainability**: It's easier to understand and debug the operations for each head when they're isolated like this.\n",
        "\n",
        "### Why do we want to reverse transpose Q, K, and V?\n",
        "\n",
        "After the attention scores are computed and used to weight $V$, we get a new tensor for each head. However, these tensors are still in the transposed shape $B \\times H \\times L \\times (D/H)$, and they need to be concatenated and linearly transformed to continue through the network. The reverse transposition essentially does the following:\n",
        "\n",
        "1. **Concatenation**: Converts the multiple heads back into a single tensor. This is required because subsequent layers (like feed-forward neural networks) expect input in the original $D$-dimensional space.\n",
        "  \n",
        "2. **Compatibility**: The rest of the neural network architecture often expects input tensors to have a specific shape (usually $B \\times L \\times D$). Reverse transposing ensures that the output of the multi-head attention block can be fed into subsequent layers without issue.\n",
        "\n",
        "3. **Resource Efficiency**: By reducing the tensor back to its original dimensions, we can save memory and computational resources, which is beneficial when you're training large models or operating under hardware constraints.\n",
        "\n",
        "In summary, the initial transposition is done to facilitate parallel computation across heads, and the reverse transposition is done to concatenate these heads and prepare the tensor for subsequent layers."
      ],
      "metadata": {
        "id": "-WX8lLDnNKYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intuition\n",
        "\n",
        "## Attention\n",
        "\n",
        "TODO: connect back later the below:\n",
        "\n",
        "1. **Attention Scores**: Once you have your $ Q, K, V $ matrices (which are all\n",
        "   $ L \\times D $ in this simplified example), you calculate the dot product\n",
        "   between queries $ Q $ and keys $ K\n",
        "   $. This is essentially\n",
        "   measuring how each word in the sentence relates to every other word.\n",
        "   Mathematically, you'll get a matrix of shape $ L \\times L $, where each\n",
        "   element $ (i, j) $ represents the \"affinity\" between the $ i^{th} $ and\n",
        "   $\n",
        "   j^{th} $ words.\n",
        "\n",
        "   **Intuition**: Imagine you're trying to understand the role of the word \"art\"\n",
        "   in the sentence. You calculate its dot product with every other word to get a\n",
        "   set of scores. These scores tell you how much each word in the sentence\n",
        "   should be \"attended to\" when you're focusing on \"art.\"\n",
        "\n",
        "2. **Scaling and Softmax**: The attention scores are scaled down by $ \\sqrt{D} $\n",
        "   and then a softmax is applied. This turns the scores into probabilities\n",
        "   (attention weights) and ensures that they sum to 1 for each word you're\n",
        "   focusing on.\n",
        "\n",
        "   **Intuition**: After scaling and softmax, you get a set of weights that tell\n",
        "   you how to create a weighted sum of all the words in the sentence when you're\n",
        "   focusing on a particular word like \"art.\"\n",
        "\n",
        "3. **Context Vector**: Finally, these attention weights are used to create a\n",
        "   weighted sum of the value vectors $ V $. This weighted sum is your context\n",
        "   vector.\n",
        "\n",
        "   **Intuition**: When focusing on the word \"art,\" you look at the attention\n",
        "   weights to decide how much of each other word you should include in your\n",
        "   understanding of \"art.\" You then sum up these weighted words to get a new\n",
        "   vector, or \"context,\" for the word \"art.\"\n",
        "\n",
        "4. **Output**: The output will be another $ L \\times D $ matrix, where each row\n",
        "   is the new \"contextualized\" representation of each word in your sentence.\n",
        "\n",
        "In your mind, you can picture it as a series of transformations: starting from\n",
        "the initial $L \\times D$ matrix, through an $ L \\times L $ attention score\n",
        "matrix and attention weights, and back to a new $ L \\times D $ context matrix.\n",
        "Each step refines the information content of your sentence, focusing on\n",
        "different relationships between the words.\n"
      ],
      "metadata": {
        "id": "dUkj6FpINRFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Walkthrough Decoder\n",
        "\n",
        "```\n",
        "Performs one decoder forward pass given encoder hidden states, the decoder input tokens and attention masks.\n",
        "B = batch size\n",
        "S = source sequence length\n",
        "T = target sequence length\n",
        "E = embedding dimensionality\n",
        "V = vocabulary size\n",
        "```\n",
        "\n",
        "## Input\n",
        "\n",
        "Let's view input's first two samples:\n",
        "\n",
        "```\n",
        "tensor([[15,  4,  9, 10,  1,  3, 13,  0,  6,  2],\n",
        "│   │   [15,  3,  5, 10,  4,  6, 13,  0,  8,  1]])\n",
        "```\n",
        "\n",
        "which is\n",
        "\n",
        "- shape is `[2, 10]` which is `BxL`.\n",
        "- `49+13=62` but no `EOS` as we truncated last token.\n",
        "- `35+46=81` but no `EOS` as we truncated last token.\n"
      ],
      "metadata": {
        "id": "mlTyEY9JNaR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dump"
      ],
      "metadata": {
        "id": "6YWMUriMOCWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Intuition.\n",
        "Jane, who loves art, visited the museum, in Paris.\n",
        "This is one sentence/sample/input.\n",
        "Ignore special char or punctuations, the input consists of 9 tokens/words.\n",
        "so L = 9. Note now in your mind picture each token/word exist in\n",
        "a D-dimensional space, like the word \"art\" can be encoded as a say,\n",
        "D=3 dimensional vector in space. This space, you can think of it as\n",
        "somewhere where words are \"grouped\" together if they are similar.\n",
        "Now the notion of similarity can be made more precise with math like\n",
        "cosine similarity, but for the sake of intuition, we can just say,\n",
        "hey, let us group nouns, verbs and adjectives together, so since art\n",
        "is a noun, it can be represented as [1,0,0] etc. Note this is grossly\n",
        "oversimplifying but you just need to get the mental model.\n",
        "So\n",
        "\n",
        "L=9, D=3.\n",
        "\n",
        "Since it is self attention we set q=k=v which is the current input.\n",
        "Now the current input is a 9 by 3 vector (L x D)! You need to picture\n",
        "this matrix in your head and know this is your 1 sample of input,\n",
        "from 1 sentence to a matrix, each row is the \"embedding\" of each token/word.\n",
        "\n",
        "---\n",
        "\n",
        "1. **Attention Scores**: Once you have your \\( Q, K, V \\) matrices (which are all \\( L \\times D \\) in this simplified example), you calculate the dot product between queries \\( Q \\) and keys \\( K \\). This is essentially measuring how each word in the sentence relates to every other word. Mathematically, you'll get a matrix of shape \\( L \\times L \\), where each element \\( (i, j) \\) represents the \"affinity\" between the \\( i^{th} \\) and \\( j^{th} \\) words.\n",
        "\n",
        "    **Intuition**: Imagine you're trying to understand the role of the word \"art\" in the sentence. You calculate its dot product with every other word to get a set of scores. These scores tell you how much each word in the sentence should be \"attended to\" when you're focusing on \"art.\"\n",
        "\n",
        "2. **Scaling and Softmax**: The attention scores are scaled down by \\( \\sqrt{D} \\) and then a softmax is applied. This turns the scores into probabilities (attention weights) and ensures that they sum to 1 for each word you're focusing on.\n",
        "\n",
        "    **Intuition**: After scaling and softmax, you get a set of weights that tell you how to create a weighted sum of all the words in the sentence when you're focusing on a particular word like \"art.\"\n",
        "\n",
        "3. **Context Vector**: Finally, these attention weights are used to create a weighted sum of the value vectors \\( V \\). This weighted sum is your context vector.\n",
        "\n",
        "    **Intuition**: When focusing on the word \"art,\" you look at the attention weights to decide how much of each other word you should include in your understanding of \"art.\" You then sum up these weighted words to get a new vector, or \"context,\" for the word \"art.\"\n",
        "\n",
        "4. **Output**: The output will be another \\( L \\times D \\) matrix, where each row is the new \"contextualized\" representation of each word in your sentence.\n",
        "\n",
        "In your mind, you can picture it as a series of transformations: starting from the initial \\( L \\times D \\) matrix, through an \\( L \\times L \\) attention score matrix and attention weights, and back to a new \\( L \\times D \\) context matrix. Each step refines the information content of your sentence, focusing on different relationships between the words.\n",
        "\"\"\"\n",
        "\n",
        "import copy\n",
        "import math\n",
        "import unittest\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import rich\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "from rich.pretty import pprint\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from src.utils.reproducibility import seed_all\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    attention: Attention\n",
        "    num_layers: int\n",
        "    vocab_size: int\n",
        "    H: int\n",
        "    d_model: int\n",
        "    d_ff: int\n",
        "    dropout: float\n",
        "    max_seq_len: int\n",
        "    bias: bool = False\n",
        "\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    __slots__ = [\n",
        "        \"d_model\",\n",
        "        \"d_k\",\n",
        "        \"d_q\",\n",
        "        \"d_v\",\n",
        "        \"H\",\n",
        "        \"W_Q\",\n",
        "        \"W_K\",\n",
        "        \"W_V\",\n",
        "        \"W_O\",\n",
        "        \"attention\",\n",
        "        \"dropout\",\n",
        "    ]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        attention: Attention,\n",
        "        H: int,\n",
        "        d_model: int,\n",
        "        dropout: float = 0.1,\n",
        "        bias: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        assert d_model % H == 0\n",
        "\n",
        "        # fmt: off\n",
        "        self.d_model   = d_model       # D\n",
        "        self.d_k       = d_model // H  # stay true to notations\n",
        "        self.d_q       = d_model // H\n",
        "        self.d_v       = d_model // H\n",
        "\n",
        "        self.H         = H             # number of heads\n",
        "\n",
        "        # shadow my notations, actually they are of shape D x D.\n",
        "        self.W_Q       = nn.Linear(self.d_model, self.d_q * self.H, bias=bias)  # D x D\n",
        "        self.W_K       = nn.Linear(self.d_model, self.d_k * self.H, bias=bias)\n",
        "        self.W_V       = nn.Linear(self.d_model, self.d_v * self.H, bias=bias)\n",
        "        self.W_O       = nn.Linear(self.d_model, self.d_model, bias=bias)\n",
        "\n",
        "        self.attention = attention\n",
        "        self.dropout   = nn.Dropout(p=dropout, inplace=False)\n",
        "        # fmt: on\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query: torch.Tensor,\n",
        "        key: torch.Tensor,\n",
        "        value: torch.Tensor,\n",
        "        mask: Optional[torch.BoolTensor] = None,\n",
        "    ) -> torch.Tensor:\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "\n",
        "        # fmt: off\n",
        "        Q = self.W_Q(query).contiguous() # Z @ W_Q -> LxD @ DxD = LxD\n",
        "        K = self.W_K(key).contiguous()   # Z @ W_K\n",
        "        V = self.W_V(value).contiguous() # Z @ W_V\n",
        "        # print(Q)\n",
        "\n",
        "        Q = self.transpose_qkv(Q)        # [B, H, L, D]\n",
        "        K = self.transpose_qkv(K)\n",
        "        V = self.transpose_qkv(V)\n",
        "        #print(Q)\n",
        "\n",
        "        # Attention\n",
        "        context_vector, attention_weights = self.attention(Q, K, V, mask)\n",
        "        context_vector_concat = self.reverse_transpose_qkv(context_vector)\n",
        "        # fmt: on\n",
        "        return self.W_O(context_vector_concat)\n",
        "\n",
        "    def transpose_qkv(self, q_or_k_or_v: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Transposition for parallel computation of multiple attention heads.\n",
        "        TODO: Why does transpose allow parallel computation?\n",
        "        \"\"\"\n",
        "        # fmt: off\n",
        "        # 1. q_or_k_or_v is shape (B, L, D)\n",
        "        # 2. aim to make it of shape (B, L, H, D / H = d_qkv)\n",
        "        batch_size, seq_len, _ = q_or_k_or_v.shape\n",
        "        q_or_k_or_v            = q_or_k_or_v.view(batch_size, seq_len, self.H, self.d_model // self.H)\n",
        "\n",
        "        # 3. switch H from 3rd to 2nd dimension, or in python swap 2nd to 1st\n",
        "        q_or_k_or_v            = q_or_k_or_v.permute(0, 2, 1, 3)\n",
        "        # fmt: on\n",
        "        return q_or_k_or_v\n",
        "\n",
        "    def reverse_transpose_qkv(self, q_or_k_or_v: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Reverse the transposition operation for concatenating multiple attention heads.\"\"\"\n",
        "        # fmt: off\n",
        "        # 1. q_or_k_or_v is shape (B, H, L, D / H = d_qkv)\n",
        "        # 2. aim to make it of shape (B, L, H, D / H = d_qkv)\n",
        "        q_or_k_or_v = q_or_k_or_v.permute(0, 2, 1, 3)\n",
        "\n",
        "        # 3. Merge H and d_qkv into D\n",
        "        batch_size, seq_len, _, _ = q_or_k_or_v.shape\n",
        "        q_or_k_or_v = q_or_k_or_v.contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        # fmt: on\n",
        "        return q_or_k_or_v\n",
        "\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "    \"\"\"residual connection: x + dropout(sublayer(layernorm(x)))\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout):\n",
        "        super().__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return x + self.drop(sublayer(self.norm(x)))\n",
        "\n",
        "\n",
        "# I simply let the model learn the positional embeddings in this notebook, since this\n",
        "# almost produces identital results as using sin/cosin functions embeddings, as claimed\n",
        "# in the original transformer paper. Note also that in the original paper, they multiplied\n",
        "# the token embeddings by a factor of sqrt(d_embed), which I do not do here.\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder = token embedding + positional embedding -> a stack of N EncoderBlock -> layer norm\"\"\"\n",
        "\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.d_model = config.d_model\n",
        "        self.tok_embed = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.pos_embed = nn.Parameter(\n",
        "            torch.zeros(1, config.max_seq_len, config.d_model)\n",
        "        )\n",
        "        self.encoder_blocks = nn.ModuleList(\n",
        "            [EncoderBlock(config) for _ in range(config.num_layers)]\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.norm = nn.LayerNorm(config.d_model)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                torch.nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, input, mask=None):\n",
        "        x = self.tok_embed(input)\n",
        "\n",
        "        x_pos = self.pos_embed[:, : x.size(1), :]\n",
        "        x = self.dropout(x + x_pos)\n",
        "        for layer in self.encoder_blocks:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class PositionWiseFFN(nn.Module):\n",
        "    \"\"\"The positionwise feed-forward network.\"\"\"\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"EncoderBlock: self-attention -> position-wise fully connected feed-forward layer\"\"\"\n",
        "\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadedAttention(\n",
        "            config.attention, config.H, config.d_model, config.dropout, config.bias\n",
        "        )\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(config.d_model, config.d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout),\n",
        "            nn.Linear(config.d_ff, config.d_model),\n",
        "        )\n",
        "        self.residual1 = ResidualConnection(config.d_model, config.dropout)\n",
        "        self.residual2 = ResidualConnection(config.d_model, config.dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # self-attention\n",
        "        x = self.residual1(x, lambda x: self.mha(x, x, x, mask=mask))\n",
        "        # position-wise fully connected feed-forward layer\n",
        "        return self.residual2(x, self.feed_forward)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config: ModelConfig, num_classes):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(config)\n",
        "        self.linear = nn.Linear(config.d_model, num_classes)\n",
        "\n",
        "    def forward(self, x, pad_mask=None):\n",
        "        x = self.encoder(x, pad_mask)\n",
        "        return self.linear(torch.mean(x, -2))\n",
        "\n",
        "\n",
        "class MultiHeadedAttentionSanity(nn.Module):\n",
        "    def __init__(self, h, d_embed, dropout=0.0):\n",
        "        super().__init__()\n",
        "        assert d_embed % h == 0  # check the h number\n",
        "        self.d_k = d_embed // h\n",
        "        self.d_embed = d_embed\n",
        "        self.h = h\n",
        "        self.WQ = nn.Linear(d_embed, d_embed)\n",
        "        self.WK = nn.Linear(d_embed, d_embed)\n",
        "        self.WV = nn.Linear(d_embed, d_embed)\n",
        "        self.linear = nn.Linear(d_embed, d_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x_query, x_key, x_value, mask=None):\n",
        "        nbatch = x_query.size(0)  # get batch size\n",
        "        # 1) Linear projections to get the multi-head query, key and value tensors\n",
        "        # x_query, x_key, x_value dimension: nbatch * seq_len * d_embed\n",
        "        # LHS query, key, value dimensions: nbatch * h * seq_len * d_k\n",
        "        query = self.WQ(x_query).view(nbatch, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        key = self.WK(x_key).view(nbatch, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        value = self.WV(x_value).view(nbatch, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        # 2) Attention\n",
        "        # scores has dimensions: nbatch * h * seq_len * seq_len\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        # 3) Mask out padding tokens and future tokens\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask, float(\"-inf\"))\n",
        "        # p_atten dimensions: nbatch * h * seq_len * seq_len\n",
        "        p_atten = torch.nn.functional.softmax(scores, dim=-1)\n",
        "        p_atten = self.dropout(p_atten)\n",
        "        # x dimensions: nbatch * h * seq_len * d_k\n",
        "        x = torch.matmul(p_atten, value)\n",
        "        # x now has dimensions:nbtach * seq_len * d_embed\n",
        "        x = x.transpose(1, 2).contiguous().view(nbatch, -1, self.d_embed)\n",
        "        return self.linear(x)  # final linear layer\n",
        "\n",
        "\n",
        "class TestAttention(unittest.TestCase):\n",
        "    def setUp(self) -> None:\n",
        "        seed_all(42, seed_torch=True)\n",
        "\n",
        "        # Initialize queries, keys, and values\n",
        "        # fmt: off\n",
        "        self.batch_size    = 2 # B\n",
        "        self.num_heads     = 2 # H\n",
        "        self.seq_len       = 4 # L\n",
        "        self.d_k           = 3\n",
        "        self.dropout       = 0.0\n",
        "        self.mask          = None\n",
        "\n",
        "        self.queries       = torch.normal(0, 1, (self.batch_size, self.seq_len, self.d_k))\n",
        "        self.keys          = torch.normal(0, 1, (self.batch_size, self.seq_len, self.d_k))\n",
        "        self.values        = torch.normal(0, 1, (self.batch_size, self.seq_len, self.d_k))\n",
        "\n",
        "        self.attention     = ScaledDotProductAttention(dropout=self.dropout)\n",
        "\n",
        "        # Initialize the attention models\n",
        "        self.attention_d2l = d2l.DotProductAttention(dropout=0.0)\n",
        "        self.attention_d2l.eval()\n",
        "        # fmt: on\n",
        "\n",
        "    def compute_attention_outputs(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        return self.attention(\n",
        "            query=self.queries, key=self.keys, value=self.values, mask=self.mask\n",
        "        )\n",
        "\n",
        "    @unittest.skip(\"TODO\")\n",
        "    def test_masking(self):\n",
        "        ...\n",
        "\n",
        "    def test_attention_weights_sum_to_one(self) -> None:\n",
        "        _, attention_weights = self.compute_attention_outputs()\n",
        "\n",
        "        # Sum the attention weights along the last dimension\n",
        "        summed_weights = torch.sum(attention_weights, dim=-1)\n",
        "\n",
        "        # Create a tensor of ones with the same shape as summed_weights for comparison\n",
        "        expected_sum = torch.ones_like(summed_weights)\n",
        "\n",
        "        # Check if the summed weights are approximately 1\n",
        "        self.assertTrue(\n",
        "            torch.allclose(summed_weights, expected_sum, atol=1e-6),\n",
        "            \"The attention weights do not sum up to 1.\",\n",
        "        )\n",
        "\n",
        "    def test_attention_output_shape_is_consistent(self) -> None:\n",
        "        # fmt: off\n",
        "        context_vector, attention_weights = self.compute_attention_outputs()\n",
        "\n",
        "        # Check the shape of the output context and attention weights\n",
        "        self.assertEqual(context_vector.shape, (self.batch_size, self.seq_len, self.d_k))\n",
        "        self.assertEqual(attention_weights.shape, (self.batch_size, self.seq_len, self.seq_len))\n",
        "        # fmt: on\n",
        "\n",
        "    def test_attention_outputs_with_d2l_as_sanity_check(self) -> None:\n",
        "        # d2l attention\n",
        "        context_vector_d2l = self.attention_d2l(\n",
        "            self.queries, self.keys, self.values, valid_lens=None\n",
        "        )\n",
        "        attention_weights_d2l = self.attention_d2l.attention_weights\n",
        "\n",
        "        # Scaled dot product attention\n",
        "        context_vector, attention_weights = self.compute_attention_outputs()\n",
        "\n",
        "        # Test if both are close\n",
        "        self.assertTrue(torch.allclose(context_vector, context_vector_d2l))\n",
        "        self.assertTrue(torch.allclose(attention_weights, attention_weights_d2l))\n",
        "\n",
        "\n",
        "class TestMultiHeadedAttention(unittest.TestCase):\n",
        "    def setUp(self) -> None:\n",
        "        seed_all(42, seed_torch=True)\n",
        "\n",
        "        self.batch_size = 128\n",
        "        self.seq_len = 100\n",
        "        self.d_model = 32\n",
        "        self.num_heads = 2\n",
        "        self.dropout = 0.0\n",
        "\n",
        "        self.query = torch.rand(self.batch_size, self.seq_len, self.d_model)\n",
        "        self.key = torch.rand(self.batch_size, self.seq_len, self.d_model)\n",
        "        self.value = torch.rand(self.batch_size, self.seq_len, self.d_model)\n",
        "\n",
        "        seed_all(42, seed_torch=True)\n",
        "        self.my_attention = MultiHeadedAttention(\n",
        "            ScaledDotProductAttention(dropout=self.dropout),\n",
        "            H=self.num_heads,\n",
        "            d_model=self.d_model,\n",
        "            dropout=self.dropout,\n",
        "            bias=True,\n",
        "        )\n",
        "        seed_all(42, seed_torch=True)\n",
        "        self.other_attention = MultiHeadedAttentionSanity(\n",
        "            h=self.num_heads, d_embed=self.d_model, dropout=self.dropout\n",
        "        )\n",
        "        # self.d2l_attention = d2l.MultiHeadAttention(\n",
        "        #     num_hiddens=self.d_model,\n",
        "        #     num_heads=self.num_heads,\n",
        "        #     dropout=self.dropout,\n",
        "        #     use_bias=False  # Match your implementation which doesn't use bias\n",
        "        # )\n",
        "\n",
        "    def test_attention_output(self):\n",
        "        my_output = self.my_attention(self.query, self.key, self.value, mask=None)\n",
        "        other_output = self.other_attention(self.query, self.key, self.value, mask=None)\n",
        "        # d2l_output = self.d2l_attention(self.query, self.key, self.value, valid_lens=None)\n",
        "        # print(my_output.shape)\n",
        "        # print(d2l_output.shape)\n",
        "        self.assertTrue(torch.allclose(my_output, other_output, atol=1e-6))\n",
        "\n",
        "\n",
        "class TestTransformerForwardPass(unittest.TestCase):\n",
        "    def test_forward_pass_output_shape(self):\n",
        "        config = ModelConfig(\n",
        "            attention=ScaledDotProductAttention(),\n",
        "            num_layers=6,\n",
        "            vocab_size=5000,\n",
        "            H=8,\n",
        "            d_model=512,\n",
        "            d_ff=2048,\n",
        "            dropout=0.1,\n",
        "            max_seq_len=1000,\n",
        "        )\n",
        "        num_classes = 10\n",
        "        model = Transformer(config, num_classes)\n",
        "\n",
        "        # Dummy input of shape (batch_size, sequence_length)\n",
        "        # rmb not 3 D data here yet because its just raw\n",
        "        input_data = torch.randint(0, 4999, (128, 100))\n",
        "\n",
        "        output = model(input_data)\n",
        "\n",
        "        # Verify if output shape is (batch_size, num_classes)\n",
        "        self.assertEqual(output.shape, (128, num_classes))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main()\n"
      ],
      "metadata": {
        "id": "Y__JKtBHOFK5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}