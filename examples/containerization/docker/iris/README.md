# Run Streamlit app in Docker

## Define Docker Volume

```bash
docker volume create iris-stores
```

## Run Streamlit Locally

```bash
export STORES_DIR=/Users/gaohn/gaohn/common-utils/examples/containerization/docker/iris/pipeline-training/stores
```

to specify the local stores directory.

Then run:

```bash
streamlit run app.py
```

If in docker, you should then pass the stores directory as a volume

```bash
docker run \
    --rm \
    -p 8501:8501 \
    -v iris-stores:/pipeline-serving/stores \
    --env STORES_DIR=/pipeline-serving/stores \
    --name iris-app \
    iris-app:v1
```

```bash
docker run \
    --rm \
    -p 8501:8501 \
    -v iris-stores:/pipeline-serving/stores \
    --name iris-app \
    iris-app:v1
```

both will work because of:

```python
self.stores_dir = (
    os.environ.get("STORES_DIR") if os.environ.get("STORES_DIR") else "./stores"
)
```

So the idea is I have defined a docker volume which can be shared by multiple
containers. It was `iris-stores`, so if you go see `iris-stores` in your local
machine, you will see the data and artifacts generated by the pipeline in
training.

Now when you mount `-v iris-stores:/pipeline-serving/stores` during serving, you
are essentially telling docker to mount the `iris-stores` volume to the
`/pipeline-serving/stores` directory in the container.

So in a way you can think of now your container has this folder:

```tree
.
|-- pipeline-serving
|   |-- stores
|   |   |-- data
|   |   |   |-- iris.csv
|   |   |-- models
```

So in a way you are mapping `iris-stores` contents exactly to
`/pipeline-serving/stores` in the container.

## Build and run

```bash
bash build.sh
```

## Streamlit Dashboard

To open the Streamlit dashboard, go to:

```bash
http://localhost:8501
```
