# Distributed Data Parallel

## Introduction: Distributed Data Parallel in PyTorch

- https://pytorch.org/tutorials/beginner/ddp_series_intro.html

## What is Distributed Data Parallel?

- https://pytorch.org/tutorials/beginner/ddp_series_theory.html

## Multi GPU Training with DDP

See `01_single_node_multi_gpu.py` for a simple example of multi GPU training with DDP.

- https://pytorch.org/tutorials/beginner/ddp_series_multigpu.html

## Fault Tolerance Distributed Training with Torch Distributed Elastic

- `torchrun` and `torch.distributed.launch`.

- https://pytorch.org/tutorials/beginner/ddp_series_fault_tolerance.html