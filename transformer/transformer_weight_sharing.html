<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>Questions for New Learners</title>
        <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

/* From extension ExecutableBookProject.myst-highlight */
:root{--color-foreground-primary: black;--color-foreground-secondary: #5a5c63;--color-foreground-border: #878787;--color-background-primary: white;--color-background-secondary: #f8f9fb;--color-background-border: #eeebee;--icon-search: url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7" /><line x1="21" y1="21" x2="15" y2="15" /></svg>');--icon-pencil: url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>');--icon-abstract: url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 5h16v2H4V5m0 4h16v2H4V9m0 4h16v2H4v-2m0 4h10v2H4v-2z"/></svg>');--icon-info: url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 9h-2V7h2m0 10h-2v-6h2m-1-9A10 10 0 002 12a10 10 0 0010 10 10 10 0 0010-10A10 10 0 0012 2z"/></svg>');--icon-flame: url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17.55 11.2c-.23-.3-.5-.56-.76-.82-.65-.6-1.4-1.03-2.03-1.66C13.3 7.26 13 4.85 13.91 3c-.91.23-1.75.75-2.45 1.32-2.54 2.08-3.54 5.75-2.34 8.9.04.1.08.2.08.33 0 .22-.15.42-.35.5-.22.1-.46.04-.64-.12a.83.83 0 01-.15-.17c-1.1-1.43-1.28-3.48-.53-5.12C5.89 10 5 12.3 5.14 14.47c.04.5.1 1 .27 1.5.14.6.4 1.2.72 1.73 1.04 1.73 2.87 2.97 4.84 3.22 2.1.27 4.35-.12 5.96-1.6 1.8-1.66 2.45-4.32 1.5-6.6l-.13-.26c-.2-.46-.47-.87-.8-1.25l.05-.01m-3.1 6.3c-.28.24-.73.5-1.08.6-1.1.4-2.2-.16-2.87-.82 1.19-.28 1.89-1.16 2.09-2.05.17-.8-.14-1.46-.27-2.23-.12-.74-.1-1.37.18-2.06.17.38.37.76.6 1.06.76 1 1.95 1.44 2.2 2.8.04.14.06.28.06.43.03.82-.32 1.72-.92 2.27h.01z"/></svg>');--icon-question: url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M15.07 11.25l-.9.92C13.45 12.89 13 13.5 13 15h-2v-.5c0-1.11.45-2.11 1.17-2.83l1.24-1.26c.37-.36.59-.86.59-1.41a2 2 0 00-2-2 2 2 0 00-2 2H8a4 4 0 014-4 4 4 0 014 4 3.2 3.2 0 01-.93 2.25M13 19h-2v-2h2M12 2A10 10 0 002 12a10 10 0 0010 10 10 10 0 0010-10c0-5.53-4.5-10-10-10z"/></svg>');--icon-warning: url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 14h-2v-4h2m0 8h-2v-2h2M1 21h22L12 2 1 21z"/></svg>');--icon-failure: url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2c5.53 0 10 4.47 10 10s-4.47 10-10 10S2 17.53 2 12 6.47 2 12 2m3.59 5L12 10.59 8.41 7 7 8.41 10.59 12 7 15.59 8.41 17 12 13.41 15.59 17 17 15.59 13.41 12 17 8.41 15.59 7z"/></svg>');--icon-spark: url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M11.5 20l4.86-9.73H13V4l-5 9.73h3.5V20M12 2c2.75 0 5.1 1 7.05 2.95C21 6.9 22 9.25 22 12s-1 5.1-2.95 7.05C17.1 21 14.75 22 12 22s-5.1-1-7.05-2.95C3 17.1 2 14.75 2 12s1-5.1 2.95-7.05C6.9 3 9.25 2 12 2z"/></svg>');--icon-admonition-default: var(--icon-abstract);--color-directive-unhandled-background: rgba(255, 255, 255, 0.3);--color-directive-error-background: rgba(255, 0, 0, 0.4);--admonition-font-size: 0.8125rem;--admonition-title-font-size: 0.8125rem;--icon-admonition-default: var(--icon-abstract);--color-admonition-background: rgba(255, 255, 255, 0.3);--color-admonition-title: #651fff;--color-admonition-title-background: rgba(101, 31, 255, 0.1);--color-admonition-title--caution: #ff9100;--color-admonition-title-background--caution: rgba(255, 145, 0, 0.1);--color-admonition-title--warning: #ff9100;--color-admonition-title-background--warning: rgba(255, 145, 0, 0.1);--color-admonition-title--danger: #ff5252;--color-admonition-title-background--danger: rgba(255, 82, 82, 0.1);--color-admonition-title--attention: #ff5252;--color-admonition-title-background--attention: rgba(255, 82, 82, 0.1);--color-admonition-title--error: #ff5252;--color-admonition-title-background--error: rgba(255, 82, 82, 0.1);--color-admonition-title--hint: #00c852;--color-admonition-title-background--hint: rgba(0, 200, 82, 0.1);--color-admonition-title--important: #00bfa5;--color-admonition-title-background--important: rgba(0, 191, 165, 0.1);--color-admonition-title--note: #00b0ff;--color-admonition-title-background--note: rgba(0, 176, 255, 0.1);--color-admonition-title--seealso: #448aff;--color-admonition-title-background--seealso: rgba(68, 138, 255, 0.1);--color-admonition-title--tip: #00c852;--color-admonition-title-background--tip: rgba(0, 200, 82, 0.1);--color-admonition-title--admonition-todo: #808080;--color-admonition-title-background--admonition-todo: rgba(128, 128, 128, 0.1)}@media(prefers-color-scheme: dark){:root{--color-foreground-primary: #ffffffd9;--color-foreground-secondary: #9ca0a5;--color-foreground-border: #666666;--color-background-primary: #131416;--color-background-secondary: #1a1c1e;--color-background-border: #303335;--color-directive-unhandled-background: #333338;--color-directive-error-background: rgba(180, 6, 6, 0.8);--color-admonition-background: #18181a}}.role-unhandled{outline:1px solid #d3d3d3;border-radius:.2rem;padding-left:.125rem;padding-right:.125rem}.role-unhandled mark{-webkit-text-decoration-line:underline;text-decoration-line:underline;background-color:unset;color:unset}.role-unhandled code{padding-left:.15rem}.directive-unhandled,.directive-error{margin:1rem auto;padding:.5rem;outline:.0625rem solid #d3d3d3;border-radius:.2rem;overflow:auto;color:var(--color-foreground-primary)}.directive-unhandled>:nth-child(2),.directive-error>:nth-child(2){margin-top:0}.directive-unhandled>:last-child,.directive-error>:last-child{margin-bottom:0}.directive-unhandled header,.directive-error header{margin-bottom:.5rem}.directive-unhandled mark,.directive-error mark{-webkit-text-decoration-line:underline;text-decoration-line:underline;background-color:unset;color:var(--color-foreground-primary)}.directive-unhandled{outline:.0625rem solid #d3d3d3;background-color:var(--color-directive-unhandled-background)}.directive-error{outline:.0625rem solid #f08080;background-color:var(--color-directive-error-background)}.admonition{margin:1rem auto;padding:0 .5rem .5rem .5rem;background:var(--color-admonition-background);color:var(--color-foreground-primary);border-radius:.2rem;border-left:.2rem solid var(--color-admonition-title);box-shadow:0 .2rem .5rem rgba(0,0,0,.05),0 0 .0625rem rgba(0,0,0,.1);font-size:var(--admonition-font-size);overflow:auto;page-break-inside:avoid}.admonition>:nth-child(2){margin-top:0}.admonition>:last-child{margin-bottom:0}.admonition p.admonition-title{position:relative;margin:0 -0.5rem .5rem;padding:.5rem .5rem .5rem 2rem;font-weight:500;font-size:var(--admonition-title-font-size);background-color:var(--color-admonition-title-background);line-height:1.3}.admonition p.admonition-title::before{content:"";position:absolute;left:.5rem;width:1rem;height:1rem;background-color:var(--color-admonition-title);-webkit-mask-image:var(--icon-admonition-default);mask-image:var(--icon-admonition-default);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat}.admonition.caution{border-left-color:var(--color-admonition-title--caution)}.admonition.caution>.admonition-title{background-color:var(--color-admonition-title-background--caution)}.admonition.caution>.admonition-title::before{background-color:var(--color-admonition-title--caution);-webkit-mask-image:var(--icon-spark);mask-image:var(--icon-spark)}.admonition.warning{border-left-color:var(--color-admonition-title--warning)}.admonition.warning>.admonition-title{background-color:var(--color-admonition-title-background--warning)}.admonition.warning>.admonition-title::before{background-color:var(--color-admonition-title--warning);-webkit-mask-image:var(--icon-warning);mask-image:var(--icon-warning)}.admonition.danger{border-left-color:var(--color-admonition-title--danger)}.admonition.danger>.admonition-title{background-color:var(--color-admonition-title-background--danger)}.admonition.danger>.admonition-title::before{background-color:var(--color-admonition-title--danger);-webkit-mask-image:var(--icon-spark);mask-image:var(--icon-spark)}.admonition.attention{border-left-color:var(--color-admonition-title--attention)}.admonition.attention>.admonition-title{background-color:var(--color-admonition-title-background--attention)}.admonition.attention>.admonition-title::before{background-color:var(--color-admonition-title--attention);-webkit-mask-image:var(--icon-warning);mask-image:var(--icon-warning)}.admonition.error{border-left-color:var(--color-admonition-title--error)}.admonition.error>.admonition-title{background-color:var(--color-admonition-title-background--error)}.admonition.error>.admonition-title::before{background-color:var(--color-admonition-title--error);-webkit-mask-image:var(--icon-failure);mask-image:var(--icon-failure)}.admonition.hint{border-left-color:var(--color-admonition-title--hint)}.admonition.hint>.admonition-title{background-color:var(--color-admonition-title-background--hint)}.admonition.hint>.admonition-title::before{background-color:var(--color-admonition-title--hint);-webkit-mask-image:var(--icon-question);mask-image:var(--icon-question)}.admonition.important{border-left-color:var(--color-admonition-title--important)}.admonition.important>.admonition-title{background-color:var(--color-admonition-title-background--important)}.admonition.important>.admonition-title::before{background-color:var(--color-admonition-title--important);-webkit-mask-image:var(--icon-flame);mask-image:var(--icon-flame)}.admonition.note{border-left-color:var(--color-admonition-title--note)}.admonition.note>.admonition-title{background-color:var(--color-admonition-title-background--note)}.admonition.note>.admonition-title::before{background-color:var(--color-admonition-title--note);-webkit-mask-image:var(--icon-pencil);mask-image:var(--icon-pencil)}.admonition.seealso{border-left-color:var(--color-admonition-title--seealso)}.admonition.seealso>.admonition-title{background-color:var(--color-admonition-title-background--seealso)}.admonition.seealso>.admonition-title::before{background-color:var(--color-admonition-title--seealso);-webkit-mask-image:var(--icon-info);mask-image:var(--icon-info)}.admonition.tip{border-left-color:var(--color-admonition-title--tip)}.admonition.tip>.admonition-title{background-color:var(--color-admonition-title-background--tip)}.admonition.tip>.admonition-title::before{background-color:var(--color-admonition-title--tip);-webkit-mask-image:var(--icon-info);mask-image:var(--icon-info)}.admonition.admonition-todo{border-left-color:var(--color-admonition-title--admonition-todo)}.admonition.admonition-todo>.admonition-title{background-color:var(--color-admonition-title-background--admonition-todo)}.admonition.admonition-todo>.admonition-title::before{background-color:var(--color-admonition-title--admonition-todo);-webkit-mask-image:var(--icon-pencil);mask-image:var(--icon-pencil)}.admonition-todo>.admonition-title{text-transform:uppercase}img{box-sizing:border-box;max-width:100%;height:auto}img.align-left,figure.align-left{clear:left;float:left;margin-right:1em}img.align-right,figure.align-right{clear:right;float:right;margin-right:1em}img.align-center,img.align-default,figure.align-center,figure.align-default{display:block;margin-left:auto;margin-right:auto}figcaption{font-style:italic;text-align:center}table{border-radius:.2rem;border-spacing:0;border-collapse:collapse;box-shadow:0 .2rem .5rem rgba(0,0,0,.05),0 0 .0625rem rgba(0,0,0,.1)}table>caption{text-align:center;margin-bottom:.25rem}th{background:var(--color-background-secondary);color:var(--color-foreground-primary)}td,th{padding:0 .25rem;border-left:1px solid var(--color-background-border);border-right:1px solid var(--color-background-border);border-bottom:1px solid var(--color-background-border)}td p,th p{margin:.25rem}td:first-child,th:first-child{border-left:none}td:last-child,th:last-child{border-right:none}

/*# sourceMappingURL=data:application/json;base64,{"version":3,"sources":["../../src/style/_variables.scss","../../src/style/_role.sass","../../src/style/_directive.sass","../../src/style/_admonition.sass","../../src/style/_image.sass","../../src/style/_tables.sass"],"names":[],"mappings":"AAwBA,MAEE,iCAAA,CACA,qCAAA,CACA,kCAAA,CAEA,iCAAA,CACA,qCAAA,CACA,kCAAA,CAIE,4UAAA,CAAA,+QAAA,CAAA,8LAAA,CAAA,gOAAA,CAAA,+wBAAA,CAAA,sZAAA,CAAA,mLAAA,CAAA,oTAAA,CAAA,2UAAA,CAEF,+CAAA,CAGA,gEAAA,CACA,wDAAA,CAGA,iCAAA,CACA,uCAAA,CACA,+CAAA,CAEA,uDAAA,CACA,iCAAA,CACA,4DAAA,CAGE,0CAAA,CACA,oEAAA,CADA,0CAAA,CACA,oEAAA,CADA,yCAAA,CACA,mEAAA,CADA,4CAAA,CACA,sEAAA,CADA,wCAAA,CACA,kEAAA,CADA,uCAAA,CACA,gEAAA,CADA,4CAAA,CACA,sEAAA,CADA,uCAAA,CACA,iEAAA,CADA,0CAAA,CACA,qEAAA,CADA,sCAAA,CACA,+DAAA,CADA,kDAAA,CACA,8EAAA,CAIJ,mCACE,MAEE,qCAAA,CACA,qCAAA,CACA,kCAAA,CAEA,mCAAA,CACA,qCAAA,CACA,kCAAA,CAGA,+CAAA,CACA,wDAAA,CAGA,sCAAA,CAAA,CC3EJ,gBACI,yBAAA,CACA,mBAAA,CACA,oBAAA,CACA,qBAAA,CAGA,qBACI,sCAAA,CAAA,8BAAA,CACA,sBAAA,CACA,WAAA,CAGJ,qBACI,mBAAA,CCdR,sCACI,gBAAA,CACA,aAAA,CACA,8BAAA,CACA,mBAAA,CACA,aAAA,CAGA,qCAAA,CAGA,kEACI,YAAA,CAGJ,8DACI,eAAA,CAEJ,oDACI,mBAAA,CAGJ,gDACI,sCAAA,CAAA,8BAAA,CACA,sBAAA,CACA,qCAAA,CAER,qBACI,8BAAA,CACA,4DAAA,CAEJ,iBACI,8BAAA,CACA,wDAAA,CChCJ,YACE,gBAAA,CACA,2BAAA,CAEA,6CAAA,CAEA,qCAAA,CAEA,mBAAA,CACA,qDAAA,CACA,oEAAA,CAEA,qCAAA,CAEA,aAAA,CACA,uBAAA,CAGA,0BACE,YAAA,CAGF,wBACE,eAAA,CAIF,+BACE,iBAAA,CACA,sBAAA,CACA,8BAAA,CAEA,eAAA,CACA,2CAAA,CACA,yDAAA,CAEA,eAAA,CAGA,uCACE,UAAA,CACA,iBAAA,CACA,UAAA,CACA,UAAA,CACA,WAAA,CAEA,8CAAA,CAEA,iDAAA,CAAA,yCAAA,CACA,6BAAA,CAAA,qBAAA,CAMF,oBACE,wDAAA,CACA,sCACE,kEAAA,CACA,8CACE,uDAAA,CACA,oCAAA,CAAA,4BAAA,CANN,oBACE,wDAAA,CACA,sCACE,kEAAA,CACA,8CACE,uDAAA,CACA,sCAAA,CAAA,8BAAA,CANN,mBACE,uDAAA,CACA,qCACE,iEAAA,CACA,6CACE,sDAAA,CACA,oCAAA,CAAA,4BAAA,CANN,sBACE,0DAAA,CACA,wCACE,oEAAA,CACA,gDACE,yDAAA,CACA,sCAAA,CAAA,8BAAA,CANN,kBACE,sDAAA,CACA,oCACE,gEAAA,CACA,4CACE,qDAAA,CACA,sCAAA,CAAA,8BAAA,CANN,iBACE,qDAAA,CACA,mCACE,+DAAA,CACA,2CACE,oDAAA,CACA,uCAAA,CAAA,+BAAA,CANN,sBACE,0DAAA,CACA,wCACE,oEAAA,CACA,gDACE,yDAAA,CACA,oCAAA,CAAA,4BAAA,CANN,iBACE,qDAAA,CACA,mCACE,+DAAA,CACA,2CACE,oDAAA,CACA,qCAAA,CAAA,6BAAA,CANN,oBACE,wDAAA,CACA,sCACE,kEAAA,CACA,8CACE,uDAAA,CACA,mCAAA,CAAA,2BAAA,CANN,gBACE,oDAAA,CACA,kCACE,8DAAA,CACA,0CACE,mDAAA,CACA,mCAAA,CAAA,2BAAA,CANN,4BACE,gEAAA,CACA,8CACE,0EAAA,CACA,sDACE,+DAAA,CACA,qCAAA,CAAA,6BAAA,CAEV,mCACE,wBAAA,CCjEF,IACI,qBAAA,CACA,cAAA,CACA,WAAA,CAGA,iCACI,UAAA,CACA,UAAA,CACA,gBAAA,CAEJ,mCACI,WAAA,CACA,WAAA,CACA,gBAAA,CAEJ,4EACI,aAAA,CACA,gBAAA,CACA,iBAAA,CAER,WACI,iBAAA,CACA,iBAAA,CCvBJ,MACI,mBAAA,CACA,gBAAA,CACA,wBAAA,CAEA,oEAAA,CAEA,cACI,iBAAA,CACA,oBAAA,CAER,GACI,4CAAA,CACA,qCAAA,CAEJ,MAGI,gBAAA,CAGA,oDAAA,CACA,qDAAA,CACA,sDAAA,CAEA,UACI,aAAA,CACJ,8BACI,gBAAA,CACJ,4BACI,iBAAA","file":"style.min.css","sourcesContent":["// This file defines all the knobs that can be tweaked by end users.\n// adapated from: https://github.com/pradyunsg/furo/blob/fe0088363a163cd9d6ffaf274560533501e935b5/src/furo/assets/styles/variables/_index.scss#L1\n\n// Admonitions\n//\n// Structure of these: (color, key-in-$icons).\n// The colors are translated into CSS variables below, and icons are used for\n// the declarations.\n$admonition-default: #651fff \"abstract\";\n$admonitions: (\n  // Each of these has a reST directives for it.\n  \"caution\": #ff9100 \"spark\",\n  \"warning\": #ff9100 \"warning\",\n  \"danger\": #ff5252 \"spark\",\n  \"attention\": #ff5252 \"warning\",\n  \"error\": #ff5252 \"failure\",\n  \"hint\": #00c852 \"question\",\n  \"important\": #00bfa5 \"flame\",\n  \"note\": #00b0ff \"pencil\",\n  \"seealso\": #448aff \"info\",\n  \"tip\": #00c852 \"info\",\n  \"admonition-todo\": #808080 \"pencil\"\n);\n\n:root {\n  // Base Colors\n  --color-foreground-primary: black; // for main text and headings\n  --color-foreground-secondary: #5a5c63; // for secondary text\n  --color-foreground-border: #878787; // for content borders\n\n  --color-background-primary: white; // for content\n  --color-background-secondary: #f8f9fb; // for navigation + ToC\n  --color-background-border: #eeebee; // for UI borders\n\n  // Icons\n  @each $name, $glyph in $icons {\n    --icon-#{$name}: #{$glyph};\n  }\n  --icon-admonition-default: var(--icon-#{nth($admonition-default, 2)});\n\n  // Directives\n  --color-directive-unhandled-background: rgba(255, 255, 255, 0.3);\n  --color-directive-error-background: rgba(255, 0, 0, 0.4);\n\n  // Admonitions\n  --admonition-font-size: 0.8125rem;\n  --admonition-title-font-size: 0.8125rem;\n  --icon-admonition-default: var(--icon-#{nth($admonition-default, 2)});\n  // Note this background is transparent in furo, but we want dark text to still work for dark backgrounds\n  --color-admonition-background: rgba(255, 255, 255, 0.3);\n  --color-admonition-title: #{nth($admonition-default, 1)};\n  --color-admonition-title-background: #{rgba(nth($admonition-default, 1), 0.1)};\n\n  @each $name, $values in $admonitions {\n    --color-admonition-title--#{$name}: #{nth($values, 1)};\n    --color-admonition-title-background--#{$name}: #{rgba(nth($values, 1), 0.1)};\n  }\n}\n\n@media (prefers-color-scheme: dark) {\n  :root {\n    // Base Colors\n    --color-foreground-primary: #ffffffd9; // for main text and headings\n    --color-foreground-secondary: #9ca0a5; // for secondary text\n    --color-foreground-border: #666666; // for content borders\n\n    --color-background-primary: #131416; // for content\n    --color-background-secondary: #1a1c1e; // for navigation + ToC\n    --color-background-border: #303335; // for UI borders\n\n    // Directives\n    --color-directive-unhandled-background: #333338;\n    --color-directive-error-background: rgba(180, 6, 6, 0.8);\n\n    // Admonitions\n    --color-admonition-background: #18181a;\n  }\n}\n",".role-unhandled\n    outline: 1px solid lightgrey\n    border-radius: 0.2rem\n    padding-left: 0.125rem\n    padding-right: 0.125rem\n\n    // the name of the role\n    mark\n        text-decoration-line: underline\n        background-color: unset\n        color: unset\n\n    // the content of the role\n    code\n        padding-left: 0.15rem\n",".directive-unhandled, .directive-error\n    margin: 1rem auto\n    padding: 0.5rem\n    outline: 0.0625rem solid lightgrey\n    border-radius: 0.2rem\n    overflow: auto\n\n    // text color\n    color: var(--color-foreground-primary)\n\n    // First element should have no margin, since the title has it.\n    > :nth-child(2)\n        margin-top: 0\n\n    // Last item should have no margin, since we'll control that w/ padding\n    > :last-child\n        margin-bottom: 0\n\n    header\n        margin-bottom: 0.5rem\n\n    // The name of the directive\n    mark\n        text-decoration-line: underline\n        background-color: unset\n        color: var(--color-foreground-primary)\n\n.directive-unhandled\n    outline: 0.0625rem solid lightgrey\n    background-color: var(--color-directive-unhandled-background)\n\n.directive-error\n    outline: 0.0625rem solid lightcoral\n    background-color: var(--color-directive-error-background)\n","// Copied from: https://github.com/pradyunsg/furo/blob/fe0088363a163cd9d6ffaf274560533501e935b5/src/furo/assets/styles/content/_admonitions.sass#L1\n.admonition\n  margin: 1rem auto\n  padding: 0 0.5rem 0.5rem 0.5rem\n\n  background: var(--color-admonition-background)\n  // copied from base/_theme.sass body\n  color: var(--color-foreground-primary)\n\n  border-radius: 0.2rem\n  border-left: 0.2rem solid var(--color-admonition-title)\n  box-shadow: 0 0.2rem 0.5rem rgba(0, 0, 0, 0.05), 0 0 0.0625rem rgba(0, 0, 0, 0.1)\n\n  font-size: var(--admonition-font-size)\n\n  overflow: auto\n  page-break-inside: avoid\n\n  // First element should have no margin, since the title has it.\n  > :nth-child(2)\n    margin-top: 0\n\n  // Last item should have no margin, since we'll control that w/ padding\n  > :last-child\n    margin-bottom: 0\n\n\n  // Defaults for all admonitions\n  p.admonition-title\n    position: relative\n    margin: 0 -0.5rem 0.5rem\n    padding: 0.5rem 0.5rem 0.5rem 2rem\n\n    font-weight: 500\n    font-size: var(--admonition-title-font-size)\n    background-color: var(--color-admonition-title-background)\n\n    line-height: 1.3\n\n    // Our fancy icon\n    &::before\n      content: \"\"\n      position: absolute\n      left: 0.5rem\n      width: 1rem\n      height: 1rem\n      // color: var(--color-admonition-title)\n      background-color: var(--color-admonition-title)\n\n      mask-image: var(--icon-admonition-default)\n      mask-repeat: no-repeat\n\n  //\n  // Variants\n  //\n  @each $type, $value in $admonitions\n    &.#{$type}\n      border-left-color: var(--color-admonition-title--#{$type})\n      > .admonition-title\n        background-color: var(--color-admonition-title-background--#{$type})\n        &::before\n          background-color: var(--color-admonition-title--#{$type})\n          mask-image: var(--icon-#{nth($value, 2)})\n\n.admonition-todo > .admonition-title\n  text-transform: uppercase\n","img\n    box-sizing: border-box\n    max-width: 100%\n    height: auto\n\nimg, figure\n    &.align-left\n        clear: left\n        float: left\n        margin-right: 1em\n\n    &.align-right\n        clear: right\n        float: right\n        margin-right: 1em\n\n    &.align-center, &.align-default\n        display: block\n        margin-left: auto\n        margin-right: auto\n\nfigcaption\n    font-style: italic\n    text-align: center\n","table\n    border-radius: 0.2rem\n    border-spacing: 0\n    border-collapse: collapse\n\n    box-shadow: 0 0.2rem 0.5rem rgba(0, 0, 0, 0.05), 0 0 0.0625rem rgba(0, 0, 0, 0.1)\n\n    & > caption\n        text-align: center\n        margin-bottom: 0.25rem\n\nth\n    background: var(--color-background-secondary)\n    color: var(--color-foreground-primary)\n\ntd,\nth\n    // Space things out properly\n    padding: 0 0.25rem\n\n    // Get the borders looking just-right.\n    border-left: 1px solid var(--color-background-border)\n    border-right: 1px solid var(--color-background-border)\n    border-bottom: 1px solid var(--color-background-border)\n\n    p\n        margin: 0.25rem\n    &:first-child\n        border-left: none\n    &:last-child\n        border-right: none\n"]} */
</style>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<link href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css" rel="stylesheet" type="text/css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
        
    </head>
    <body class="vscode-body vscode-light">
        <h1 id="questions-for-new-learners">Questions for New Learners</h1>
<h2 id="qn-1-if-there-are-more-than-1-sample-does-it-mean-for-each-sample-there-is-a-different-weight-matrix-for-q-k-and-v">Qn 1 If there are more than 1 sample, does it mean for each sample, there is a different weight matrix for Q, K and V?</h2>
<blockquote>
<p>If there are more than 1 sample, does it mean for each sample, there is a
different weight matrix for Q, K and V?</p>
</blockquote>
<p>No, you are not correct in saying that for each sample there is a different
weight in the transformer model. The weights used to transform the input
embeddings into the query (Q), key (K), and value (V) matrices are shared across
all samples within the batch.</p>
<p>The shared weights enable the model to generalize across different samples,
allowing it to apply the learned relationships and patterns to unseen data. This
is consistent with the principles of weight sharing in deep learning, where the
same parameters are used across different parts of the input data, ensuring that
the model learns consistent representations.</p>
<ol>
<li>
<p><strong>Shared Weights for Q, K, V:</strong></p>
<ul>
<li>The matrices Q, K, and V are derived from the input embeddings using three
different weight matrices <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>K</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">W_Q, W_K, W_V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, respectively.</li>
<li>These weight matrices are parameters that are learned during training, and
they are shared across all samples in a batch.</li>
<li>The dimensions of these matrices are the same, e.g., <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub><mo>:</mo><mo stretchy="false">[</mo><mi>d</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">W_Q: [d, d]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal">d</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">d</span><span class="mclose">]</span></span></span></span> if the
input and query dimensions are both <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">d</span></span></span></span>.</li>
</ul>
</li>
<li>
<p><strong>Batch Processing:</strong></p>
<ul>
<li>Suppose you have a batch of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> samples, each with sequence length <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span>.</li>
<li>The input embeddings for the entire batch have dimensions <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>N</mi><mo separator="true">,</mo><mi>T</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[N, T, d]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">d</span><span class="mclose">]</span></span></span></span>.</li>
<li>When you multiply the input embeddings with the weight matrices
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>K</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">W_Q, W_K, W_V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, you use the same weights for every sample in the batch.</li>
<li>So, the transformed Q, K, V matrices for the entire batch would have
dimensions <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>N</mi><mo separator="true">,</mo><mi>T</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[N, T, d]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">d</span><span class="mclose">]</span></span></span></span>, using the same <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>K</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">W_Q, W_K, W_V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> for each sample.</li>
</ul>
</li>
<li>
<p><strong>Why Share Weights?</strong></p>
<ul>
<li>Sharing weights allows the model to generalize patterns learned across
different samples.</li>
<li>If each sample had different weights, the model would have a massive
number of parameters, leading to overfitting and poor generalization to
unseen data.</li>
<li>Weight sharing simplifies the model and enforces that the same
transformation is applied to all samples, which helps the model learn
consistent relationships across different sequences.</li>
</ul>
</li>
<li>
<p><strong>Backpropagation:</strong></p>
<ul>
<li>During training, the shared weights are updated based on the aggregated
loss across the entire batch.</li>
<li>Gradients are computed with respect to this combined loss, and the
optimization algorithm updates the shared weights accordingly.</li>
<li>Even though the weights are updated based on all samples in the batch,
they remain shared across all samples.</li>
</ul>
</li>
</ol>
<p>In summary, the concept of shared weights is vital in the transformer
architecture. The same weight matrices are used to transform the input
embeddings into the Q, K, V matrices for every sample in a batch. This approach
allows the model to generalize well, prevents overfitting, and makes the
training process more computationally efficient.</p>
<h2 id="qn-2">Qn 2</h2>
<p>Just like our good old image classification</p>
<p>The concept of shared weights is also true in the context of image
classification, particularly in the use of convolutional neural networks (CNNs).</p>
<ol>
<li>
<p><strong>Convolutional Layers:</strong></p>
<ul>
<li>In a CNN, the convolutional layers consist of a set of learnable filters
or kernels.</li>
<li>These filters are applied to different regions of the input image,
effectively scanning the entire image to detect specific features like
edges, textures, or more complex patterns.</li>
<li>The key point is that the same filter is used across the entire image.
This means the weights within that filter are shared across different
spatial locations of the input.</li>
</ul>
</li>
<li>
<p><strong>Benefits of Weight Sharing:</strong></p>
<ul>
<li>The sharing of weights drastically reduces the number of parameters, as
you don't need a separate set of weights for each location in the image.</li>
<li>This reduction in parameters not only makes the model more computationally
efficient but also helps prevent overfitting.</li>
<li>Weight sharing enables the model to detect a specific feature regardless
of its location in the image, contributing to the translation invariance
of the model.</li>
</ul>
</li>
<li>
<p><strong>Pooling Layers:</strong></p>
<ul>
<li>Pooling layers, often used in CNNs, also exhibit a form of weight sharing.
A pooling operation (e.g., max pooling) applies the same operation to
different regions of the feature map, further contributing to translation
invariance.</li>
</ul>
</li>
<li>
<p><strong>Fully Connected Layers:</strong></p>
<ul>
<li>In the fully connected layers of a CNN, the weights are not shared across
different parts of the input, as they are in the convolutional layers.</li>
<li>However, the principles of batching and shared weights across different
samples in a batch still apply.</li>
</ul>
</li>
</ol>
<p>In summary, the concept of weight sharing in image classification, specifically
within CNNs, is central to their efficiency and effectiveness. By using the same
set of weights across different regions of the input image, CNNs can learn
spatial hierarchies of features with a manageable number of parameters, enabling
them to generalize well to unseen images.</p>
<h3 id="so-if-i-have-4-images-in-a-batch-the-weight-is-the-same-weight-and-not-4-different-weights">So if I have 4 images in a batch the weight is the same weight and not 4 different weights</h3>
<p>If you have a batch of 4 images and you are using a neural network model such as
a Convolutional Neural Network (CNN) for image classification, the weights
within the layers of the network are shared across all 4 images in the batch.</p>
<p>In other words, the same filters in the convolutional layers, the same weights
in the fully connected layers, and other learnable parameters are applied to
each of the 4 images. There are not 4 different sets of weights for the 4
different images.</p>
<p>This weight sharing across different samples in the batch is what enables the
model to generalize across various instances of data and is fundamental to the
training of neural networks. It ensures that the patterns and features learned
from one image are applicable to other images as well, contributing to the
model's ability to classify unseen images accurately.</p>
<h3 id="so-if-my-image-has-40-images-and-batch-size-of-4-will-there-be-8-different-weights">So if my image has 40 images and batch size of 4. Will there be 8 different weights</h3>
<p>No, even if your dataset consists of 40 images and you are training with a batch
size of 4, there will not be 8 different sets of weights for the 8 different
batches. The same weights are used across all batches and all images in the
entire dataset.</p>
<p>Here's how the process works:</p>
<ol>
<li>
<p><strong>Initialization</strong>: The weights in the model (e.g., filters in convolutional
layers, weights in fully connected layers) are initialized, often randomly.</p>
</li>
<li>
<p><strong>Batch Processing</strong>: You divide the 40 images into 8 batches, each
containing 4 images.</p>
<ul>
<li>For each batch, the forward pass computes the predictions using the same
weights for all 4 images in the batch.</li>
<li>The loss is computed for that batch, and then the backpropagation
algorithm calculates the gradients based on that loss.</li>
<li>The gradients are used to update the weights.</li>
</ul>
</li>
<li>
<p><strong>Weight Updates</strong>: The key point is that these weight updates are applied to
the same set of weights, regardless of which batch you are processing. There
are not separate weights for each batch.</p>
</li>
<li>
<p><strong>Iterative Process</strong>: As you iterate through multiple epochs, continually
cycling through the batches, the weights continue to be updated in a way that
minimizes the loss on the entire dataset of 40 images.</p>
</li>
<li>
<p><strong>Generalization</strong>: By using the same weights across all images, the model
learns to generalize the patterns and features that are relevant for the
classification task across the entire dataset.</p>
</li>
</ol>
<p>So, in summary, there is only one set of weights in the model, and these weights
are shared across all 40 images, regardless of how they are batched. The concept
of weight sharing is fundamental to neural network training, and it applies
across all images and all batches.</p>
<h3 id="after-training-transformer-is-the-q-weight-matrix-wq-fixed">After training transformer is the Q weight matrix WQ fixed</h3>
<p>Yes, after training a transformer model, the weight matrix <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">W_Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> used to
generate the query (Q) representations becomes fixed. The same applies to the
weight matrices <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">W_K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">W_V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> used for the keys (K) and values (V),
respectively.</p>
<p>During the training phase, these weight matrices are learned and optimized to
minimize the loss function. They are updated iteratively based on the gradients
computed during backpropagation.</p>
<p>Once training is complete, the weights are no longer updated, and they become
fixed parameters of the model. These fixed weights can then be used to generate
Q, K, and V for any given input when using the model for inference (i.e., making
predictions on unseen data).</p>
<p>If you want to further adapt or fine-tune the model on new data, you would
continue the training process, and the weights would be updated accordingly. But
in a standard training scenario where you train the model and then use it for
inference, the weights, including <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">W_Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>, become fixed after training is
finished.</p>
<h3 id="then-how-does-it-learn-context-when-i-pass-in-new-sentence-if-weight-is-fixed">Then how does it learn context when I pass in new sentence if weight is fixed</h3>
<p>The ability to capture context when processing a new sentence, even with fixed
weights, is a result of the design of the transformer architecture. Here's how
it works:</p>
<ol>
<li>
<p><strong>Learned Representations</strong>: During training, the transformer learns to
create useful representations of the input data by optimizing the weights,
including <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">W_Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">W_K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">W_V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>. These representations capture the
relationships and patterns in the training data.</p>
</li>
<li>
<p><strong>Contextual Encoding</strong>: The fixed weights are applied to new sentences to
generate Q, K, and V matrices. The self-attention mechanism then uses these
matrices to compute attention scores, allowing each token in the sentence to
attend to every other token. This captures the contextual relationships
within the sentence.</p>
</li>
<li>
<p><strong>Positional Encoding</strong>: Transformers include positional encodings, which are
added to the input embeddings to provide information about the position of
tokens in the sequence. This ensures that the model can recognize the order
of words, which is essential for understanding syntax and semantics.</p>
</li>
<li>
<p><strong>Layering and Stacking</strong>: The transformer consists of multiple layers of
self-attention and feed-forward neural networks. As the input progresses
through these layers, it becomes increasingly abstract and contextualized.
The higher layers are capable of capturing more complex relationships and
semantics.</p>
</li>
<li>
<p><strong>Generalization</strong>: The fixed weights have been trained to generalize from
the training data to unseen data. When you pass a new sentence through the
trained model, it uses the learned weights to encode the new sentence in a
way that reflects the patterns and relationships it learned during training.
This allows it to understand and generate context-aware representations of
new sentences.</p>
</li>
<li>
<p><strong>No Memory of Previous Sentences</strong>: It's important to note that transformers
do not have a memory of previous sentences (they are not stateful like RNNs).
The context they capture is within the current input sentence or sequence of
sentences. They do not use information from previous unrelated inputs.</p>
</li>
</ol>
<p>In summary, the fixed weights in a trained transformer model are used to apply
the learned patterns and relationships to new sentences. The model's
architecture, including the self-attention mechanism and positional encoding,
enables it to capture the context within a given sentence or sequence. Even with
fixed weights, the transformer can understand and respond to the contextual
relationships in new inputs.</p>
<h3 id="but-weight-matrix-is-lets-say-dxd-right-it-does-not-take-into-account-the-vocabulary-words-how-it-knows-context">But weight matrix is let’s say DxD right it does not take into account the vocabulary words how it knows context</h3>
<p>It is correct that the weight matrices for the queries (Q), keys (K), and values
(V), such as the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>×</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">D \times D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span> matrix <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">W_Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>, don't directly take into account the
vocabulary words. The contextual understanding in transformers comes from the
combination of the architecture's design and the processing of input embeddings.
Here's a step-by-step explanation of how it works:</p>
<ol>
<li>
<p><strong>Input Embeddings</strong>: Each word (or subword) from the vocabulary is first
mapped to a continuous vector using an embedding layer. These embeddings are
typically of size <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span></span></span></span>, and they capture the semantic meaning of the words.</p>
</li>
<li>
<p><strong>Positional Encoding</strong>: Positional encodings are added to the embeddings to
give the model information about the order of the words in the sentence.</p>
</li>
<li>
<p><strong>Linear Transformation</strong>: The embeddings are then linearly transformed using
the weight matrices <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">W_Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">W_K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">W_V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>. These transformations project
the embeddings into a space where the self-attention mechanism can operate.</p>
</li>
<li>
<p><strong>Self-Attention Mechanism</strong>:</p>
<ul>
<li>The queries (Q), keys (K), and values (V) are used to calculate attention
scores.</li>
<li>The attention scores determine how much each word in the sentence should
pay attention to every other word.</li>
<li>This attention mechanism allows the model to capture relationships between
words, regardless of their position in the sentence, thus understanding
the context.</li>
</ul>
</li>
<li>
<p><strong>Aggregated Context</strong>: The attention scores are used to take a weighted sum
of the value vectors (V), creating an aggregated representation for each word
that takes into account the entire context of the sentence.</p>
</li>
<li>
<p><strong>Layering</strong>: As the information passes through multiple layers of the
transformer, these context-aware representations are further refined,
allowing the model to capture more complex relationships and semantics.</p>
</li>
<li>
<p><strong>No Direct Vocabulary Dependence</strong>: The weight matrices themselves don't
have a direct dependence on specific words in the vocabulary. Instead, they
operate on continuous vector representations, and the understanding of
context comes from the architecture's ability to model relationships between
these vectors.</p>
</li>
</ol>
<p>In summary, the transformer doesn't &quot;know&quot; the context through the weight
matrices alone but through the combination of embeddings, linear
transformations, and the self-attention mechanism. The weight matrices are part
of this process, transforming the embeddings in a way that allows the model to
effectively apply self-attention and capture the contextual relationships within
the input sentence.</p>
<h3 id="so-in-a-sense-once-u-train-a-model-all-weights-are-fixed-and-weights-are-shared-across-all-data-samples">So in a sense once u train a model all weights are fixed. And weights are shared across ALL data samples</h3>
<p>Yes. Once you train a model, such as a transformer or a convolutional neural
network, all the weights are fixed unless you decide to fine-tune or continue
training on new data. These weights include not only those used in the
self-attention mechanism of a transformer but also any other learnable
parameters in the network.</p>
<p>Furthermore, the weights are shared across all data samples. This means that the
same set of weights is used to process every sample in your dataset, whether
it's during training or inference.</p>
<p>This weight-sharing across samples is a fundamental aspect of how these models
generalize from the training data to unseen data. By using the same weights for
all samples, the model learns to recognize patterns and relationships that are
consistent across different instances of the data, allowing it to make accurate
predictions or representations even for new, unseen samples.</p>
<h3 id="so-if-i-have-40-samples-and-batch-size-8-they-accumulate-the-loss-and-back-prop-to-update-the-same-weight-matrix-for-all-40-samples-in">So if I have 40 samples and batch size 8 they accumulate the loss and back prop to update the same weight matrix for all 40 samples in</h3>
<p>Yes, that's correct. If you have 40 samples and a batch size of 8, the training
process would generally proceed as follows:</p>
<ol>
<li>
<p><strong>Divide into Batches</strong>: The 40 samples are divided into 5 batches, each
containing 8 samples.</p>
</li>
<li>
<p><strong>Forward Pass</strong>: For each batch, the model performs a forward pass,
processing the 8 samples using the same weight matrix and other parameters.
The predictions are compared to the true labels to compute the loss for that
batch.</p>
</li>
<li>
<p><strong>Accumulate Gradients</strong>: The gradients of the loss with respect to the
weights are computed using backpropagation. Depending on the optimization
algorithm, these gradients might be accumulated across batches or used
immediately to update the weights.</p>
</li>
<li>
<p><strong>Update Weights</strong>: The accumulated gradients are used to update the same
weight matrix and other learnable parameters in the model. This update is
done according to the optimization algorithm used, such as stochastic
gradient descent (SGD) or one of its variants like Adam.</p>
</li>
<li>
<p><strong>Iterate</strong>: This process is repeated for each of the 5 batches, cycling
through the entire dataset. The weights continue to be updated based on the
accumulated gradients from all the samples.</p>
</li>
<li>
<p><strong>Multiple Epochs</strong>: Often, this entire process is repeated for multiple
epochs, meaning that the entire dataset is passed through the network
multiple times, continually updating the weights to minimize the loss.</p>
</li>
</ol>
<p>The key is that the same weight matrix and other parameters are used for all 40
samples and updated based on the loss computed over all of them. By processing
the data in batches and updating the weights based on the accumulated gradients,
the model is able to learn from the entire dataset, generalizing the patterns
and relationships in the data to make accurate predictions on unseen samples.</p>
<h3 id="so-the-idea-is-to-learn-fixed-weights-that-represent-whole-data-set">So the idea is to learn fixed weights that represent whole data set</h3>
<p>Yes, exactly. The goal of training a neural network, including models like
transformers, is to learn a set of fixed weights that effectively represent the
underlying patterns and relationships in the entire dataset. These weights are
optimized to minimize a loss function, which quantifies the difference between
the model's predictions and the true labels or values.</p>
<p>By learning fixed weights that capture the general characteristics of the data,
the model can:</p>
<ol>
<li>
<p><strong>Generalize to Unseen Data</strong>: The learned weights allow the model to make
predictions or representations for new, unseen samples that were not part of
the training dataset. The model's ability to generalize is crucial for its
performance on real-world tasks.</p>
</li>
<li>
<p><strong>Share Weights Across Samples</strong>: The same set of weights is used for all
samples in the dataset, both during training and inference. This
weight-sharing enables the model to recognize consistent patterns across
different instances of the data.</p>
</li>
<li>
<p><strong>Efficiently Represent Complex Relationships</strong>: Through the architecture's
design and the training process, the model learns to encode complex
relationships and semantics within the fixed weights. This allows it to
understand and process new inputs in a context-aware manner, even though the
weights are no longer changing after training.</p>
</li>
<li>
<p><strong>Apply to Various Tasks</strong>: Once trained, the fixed weights can be used for
various tasks, such as classification, regression, translation, or other
forms of prediction, depending on the model's design and training objective.</p>
</li>
</ol>
<p>In summary, the idea of learning fixed weights that represent the whole dataset
is central to the training and functioning of neural network models. It allows
them to capture and generalize the essential characteristics of the data,
enabling accurate and meaningful predictions or representations for a wide range
of applications.</p>
<h3 id="you-imagine-your-dataset-has-2-samples-cat-eat-mouse-and-mouse-eat-cat-the-eat-word-embedding-is-captured-such-that-the-model-knows-both-context-so-the-weight-matrix-is-also-adjusted">You imagine your dataset has 2 samples cat eat mouse and mouse eat cat, the eat word embedding is captured such that the model knows both context, so the weight matrix is also adjusted</h3>
<p>Yes. In the scenario you described, with the two samples &quot;cat eat mouse&quot; and
&quot;mouse eat cat,&quot; the model would learn to understand the context in which the
word &quot;eat&quot; appears, even though it's common to both sentences.</p>
<p>Here's how it would typically work:</p>
<ol>
<li>
<p><strong>Word Embeddings</strong>: The individual words are first mapped to continuous
vector representations (embeddings). The embedding for the word &quot;eat&quot; may
start with a pre-trained or randomly initialized value.</p>
</li>
<li>
<p><strong>Contextual Understanding</strong>: Through the training process, the model's
architecture, such as the self-attention mechanism in a transformer, allows
it to recognize the relationships between the words in each sentence. It
understands that &quot;eat&quot; has different subjects and objects in the two samples.</p>
</li>
<li>
<p><strong>Weight Adjustment</strong>: The weights, including those in the matrices used for
the self-attention mechanism, are adjusted during training to capture these
contextual relationships. They are optimized to minimize the loss across both
samples, ensuring that the model learns to recognize the different contexts
in which &quot;eat&quot; appears.</p>
</li>
<li>
<p><strong>Generalized Representations</strong>: As a result, the model learns to represent
the word &quot;eat&quot; in a way that takes into account its context within each
sentence. The fixed weights capture this understanding, allowing the model to
recognize and respond to the varying relationships between words, even in
unseen data.</p>
</li>
<li>
<p><strong>Weight Sharing</strong>: The same set of weights is used to process both samples,
and all other samples in the dataset, allowing the model to generalize the
patterns and relationships it learns.</p>
</li>
</ol>
<p>In this way, the model learns to represent the word &quot;eat&quot; (and all other words
in the vocabulary) in a context-aware manner. The fixed weights that are learned
during training enable it to understand the different ways that &quot;eat&quot; relates to
other words in the sentences, recognizing both &quot;cat eat mouse&quot; and &quot;mouse eat
cat&quot; as valid and distinct constructions.</p>
<h3 id="kernels-and-multihead-can-you-explain-the-connection-of-image-filters-and-kernels-and-transformers-multi-head-are-they-related">Kernels and Multihead Can you explain the connection of image filters and kernels and transformers multi head, are they related</h3>
<p>While image filters and kernels in convolutional neural networks (CNNs) and the
multi-head attention mechanism in transformers have different purposes and
architectures, they do share some underlying principles and connections. Here's
how they relate:</p>
<h3 id="1-local-and-global-relationships">1. <strong>Local and Global Relationships</strong></h3>
<ul>
<li><strong>Image Filters and Kernels (CNNs)</strong>: Convolutional filters (also known as
kernels) capture local spatial patterns in images. By sliding over the input
image, they learn to detect features such as edges, textures, and shapes
within a localized region.</li>
<li><strong>Multi-Head Attention (Transformers)</strong>: Multi-head attention allows the
model to focus on different parts of the input simultaneously, capturing
both local and global relationships between words. Each head can attend to
different relationships, allowing for a more nuanced understanding of the
context.</li>
</ul>
<h3 id="2-weight-sharing">2. <strong>Weight Sharing</strong></h3>
<ul>
<li><strong>CNNs</strong>: Convolutional kernels share weights across spatial locations,
allowing the network to recognize the same feature regardless of its
position in the image.</li>
<li><strong>Transformers</strong>: In multi-head attention, the weights are shared across the
entire sequence, allowing the model to recognize patterns and relationships
regardless of the position of words in the sequence.</li>
</ul>
<h3 id="3-multiple-representations">3. <strong>Multiple Representations</strong></h3>
<ul>
<li><strong>CNNs</strong>: Different filters learn to recognize different features within an
image, leading to multiple feature maps that collectively represent various
aspects of the image.</li>
<li><strong>Transformers</strong>: Different attention heads in multi-head attention can
learn to focus on different relationships within the input, providing
multiple, parallel representations of the context.</li>
</ul>
<h3 id="4-compositionality">4. <strong>Compositionality</strong></h3>
<ul>
<li><strong>CNNs</strong>: Hierarchical stacking of convolutional layers allows the model to
build increasingly complex and abstract representations of the image,
starting from simple edges to higher-level concepts like objects.</li>
<li><strong>Transformers</strong>: The layered structure of transformers also allows for
increasingly complex and abstract representations of the input, starting
from individual word embeddings to higher-level syntactic and semantic
understandings.</li>
</ul>
<h3 id="5-dimensionality-reduction-and-expansion">5. <strong>Dimensionality Reduction and Expansion</strong></h3>
<ul>
<li><strong>CNNs</strong>: Pooling layers and striding can reduce the spatial dimensions,
while different numbers of filters can expand the feature dimensions.</li>
<li><strong>Transformers</strong>: The dimensionality of the attention heads and the number
of heads can be configured to control the size and complexity of the
representations.</li>
</ul>
<p>In summary, while the specific mechanisms and purposes differ between image
filters and kernels in CNNs and the multi-head attention in transformers, they
do share principles of capturing local and global relationships, weight sharing,
multiple parallel representations, compositionality, and control over
dimensionality. These connections reflect broader themes in deep learning, where
different architectures leverage similar concepts to learn from various types of
data.</p>

        <script async src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script>
        
    </body>
    </html>